"""
ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ - æ”¯æŒOpenAIå½¢å¼çš„å¤šæä¾›å•†è°ƒç”¨
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import Config

import time
import json
import logging
from typing import List, Dict, Any, Optional, Union
from openai import OpenAI
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLMConfig:
    """LLMé…ç½®ç®¡ç†ç±»"""
    
    # é»˜è®¤å‚æ•°
    DEFAULT_TEMPERATURE = 0.3
    DEFAULT_TOP_P = 0.95
    DEFAULT_MAX_TOKENS = 4096
    
    @staticmethod
    def get_provider_config(provider: str) -> Dict[str, Any]:
        """è·å–æŒ‡å®šæä¾›å•†çš„é…ç½®"""
        configs = {
            "gemini": {
                "api_key": Config.GEMINI_API_KEY,
                "model": Config.GEMINI_MODEL,
                "base_url": "https://generativelanguage.googleapis.com/v1beta",
                "temperature": Config.DEFAULT_TEMPERATURE,
                "top_p": Config.DEFAULT_TOP_P
            },
            "openai": {
                "api_key": Config.OPENAI_API_KEY,
                "model": "gpt-4",
                "base_url": "https://api.openai.com/v1",
                "temperature": Config.DEFAULT_TEMPERATURE,
                "top_p": Config.DEFAULT_TOP_P
            }
        }
        
        if provider not in configs:
            raise ValueError(f"Unsupported provider: {provider}")
        
        return configs[provider]


class ChatMessage:
    """èŠå¤©æ¶ˆæ¯ç±»"""
    
    def __init__(self, role: str, content: str, timestamp: Optional[float] = None):
        self.role = role  # "user", "assistant", "system"
        self.content = content
        self.timestamp = timestamp or time.time()
    
    def to_dict(self) -> Dict[str, str]:
        return {
            "role": self.role,
            "content": self.content
        }


class LLMClient:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯"""
    
    def __init__(self, default_provider: str = "gemini"):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            default_provider: é»˜è®¤çš„LLMæä¾›å•†
        """
        self.default_provider = default_provider
        self.chat_history: List[ChatMessage] = []
        
        # åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯
        self._init_gemini()
    
    def _init_gemini(self):
        """åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯"""
        try:
            config = LLMConfig.get_provider_config("gemini")
            if config.get("api_key"):
                genai.configure(api_key=config["api_key"])
                logger.info("âœ… Gemini client initialized")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to initialize Gemini: {e}")
    
    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°èŠå¤©å†å²"""
        message = ChatMessage(role, content)
        self.chat_history.append(message)
    
    def get_chat_history(self) -> List[Dict[str, str]]:
        """è·å–èŠå¤©å†å²ï¼ˆOpenAIæ ¼å¼ï¼‰"""
        return [msg.to_dict() for msg in self.chat_history]
    
    def clear_history(self):
        """æ¸…ç©ºèŠå¤©å†å²"""
        self.chat_history = []
        logger.info("ğŸ—‘ï¸ Chat history cleared")
    
    def chat(self, user_message: str, provider: Optional[str] = None, 
             model: Optional[str] = None, temperature: Optional[float] = None, 
             top_p: Optional[float] = None, thinking_budget: Optional[int] = None,
             include_thoughts: Optional[bool] = None) -> str:
        """ç®€å•çš„èŠå¤©æ¥å£

        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            provider: æŒ‡å®šçš„LLMæä¾›å•†ï¼ˆå¯é€‰ï¼‰
            model: æŒ‡å®šçš„æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰ï¼Œä¼šè¦†ç›–configä¸­çš„é»˜è®¤æ¨¡å‹
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: Top-pé‡‡æ ·å‚æ•°
            thinking_budget: æ€è€ƒé¢„ç®—ï¼ˆä»…geminiæ”¯æŒï¼‰
            include_thoughts: æ˜¯å¦åŒ…å«æ€è€ƒè¿‡ç¨‹ï¼ˆä»…geminiæ”¯æŒï¼‰

        Returns:
            åŠ©æ‰‹çš„å“åº”
        """
        try:
            # ä½¿ç”¨æŒ‡å®šæä¾›å•†æˆ–é»˜è®¤æä¾›å•†
            provider = provider or self.default_provider
            
            # è·å–æä¾›å•†é…ç½®
            config = LLMConfig.get_provider_config(provider)
            
            # ä½¿ç”¨æŒ‡å®šæ¨¡å‹æˆ–é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹
            model_name = model or config.get('model')
            
            # ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°æˆ–ä¼ å…¥çš„å‚æ•°
            temperature = temperature or config.get('temperature', LLMConfig.DEFAULT_TEMPERATURE)
            top_p = top_p or config.get('top_p', LLMConfig.DEFAULT_TOP_P)

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°èŠå¤©å†å²
            self.add_message("user", user_message)

            # è°ƒç”¨LLM
            start_time = time.time()
            
            logger.info(f"å¼€å§‹è°ƒç”¨ {provider} ({model_name})...")

            if provider == "gemini":
                result = self._call_gemini_direct(
                    user_message, model_name, temperature, top_p, 
                    thinking_budget, include_thoughts
                )
            else:
                # ä½¿ç”¨OpenAIå…¼å®¹æ¥å£
                client = OpenAI(
                    api_key=config["api_key"],
                    base_url=config["base_url"]
                )

                # æ„é€ åŸºç¡€è¯·æ±‚å‚æ•°
                request_params = {
                    "model": model_name,
                    "messages": self.get_chat_history(),
                    "temperature": temperature,
                    "top_p": top_p
                }

                # å¦‚æœæ˜¯geminiæä¾›å•†ä¸”è®¾ç½®äº†thinking_budgetï¼Œæ·»åŠ specialæ ¼å¼
                if provider == "gemini" and thinking_budget is not None:
                    # å¦‚æœæ²¡æœ‰æŒ‡å®šinclude_thoughtsï¼Œé»˜è®¤ä¸ºFalse
                    if include_thoughts is None:
                        include_thoughts = False
                    
                    request_params["extra_body"] = {
                        "generationConfig": {
                            "responseSchema": {
                                "type": "object",
                                "properties": {
                                    "thinking": {
                                        "type": "string"
                                    },
                                    "response": {
                                        "type": "string"
                                    }
                                }
                            }
                        }
                    }

                response = client.chat.completions.create(**request_params)
                result = response.choices[0].message.content

            elapsed = time.time() - start_time
            logger.info(f'{provider} ({model_name}) å“åº”æ—¶é—´: {elapsed:.2f}s')

            # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°èŠå¤©å†å²
            self.add_message("assistant", result)

            return result

        except Exception as e:
            logger.error(f"è°ƒç”¨ {provider} ({model_name if 'model_name' in locals() else 'unknown'}) å¤±è´¥: {str(e)}")
            raise
    
    def _call_gemini_direct(self, message: str, model_name: str, 
                           temperature: float, top_p: float,
                           thinking_budget: Optional[int] = None,
                           include_thoughts: Optional[bool] = None) -> str:
        """ç›´æ¥è°ƒç”¨Gemini API"""
        try:
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model = genai.GenerativeModel(
                model_name=model_name,
                safety_settings={
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                }
            )
            
            # å‡†å¤‡ç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "top_p": top_p,
                "max_output_tokens": LLMConfig.DEFAULT_MAX_TOKENS,
            }
            
            # å¦‚æœè®¾ç½®äº†thinking_budgetï¼Œæ·»åŠ ç›¸å…³é…ç½®
            if thinking_budget is not None:
                # Gemini 2.5çš„æ€è€ƒæ¨¡å¼é…ç½®
                generation_config["response_format"] = {
                    "type": "json_object" if include_thoughts else "text"
                }
            
            # ç”Ÿæˆå“åº”
            response = model.generate_content(
                message,
                generation_config=generation_config
            )
            
            if response.candidates and len(response.candidates) > 0:
                result = response.candidates[0].content.parts[0].text
                
                # å¦‚æœå¯ç”¨äº†æ€è€ƒè¿‡ç¨‹ï¼Œè§£æJSONå“åº”
                if thinking_budget is not None and include_thoughts:
                    try:
                        parsed_response = json.loads(result)
                        thinking = parsed_response.get("thinking", "")
                        actual_response = parsed_response.get("response", result)
                        
                        if thinking:
                            logger.info(f"ğŸ¤” Gemini thinking: {thinking[:100]}...")
                        
                        return actual_response
                    except json.JSONDecodeError:
                        logger.warning("âš ï¸ Failed to parse thinking response, using raw text")
                
                return result
            else:
                raise Exception("No valid response generated")
                
        except Exception as e:
            logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def simple_query(self, query: str, provider: Optional[str] = None, 
                    model: Optional[str] = None) -> str:
        """ç®€å•çš„å•æ¬¡æŸ¥è¯¢ï¼Œä¸ä¿ç•™å†å²è®°å½•"""
        original_history = self.chat_history.copy()
        self.clear_history()
        
        try:
            response = self.chat(query, provider=provider, model=model)
            return response
        finally:
            # æ¢å¤åŸå§‹å†å²è®°å½•
            self.chat_history = original_history
    
    def batch_query(self, queries: List[str], provider: Optional[str] = None,
                   model: Optional[str] = None) -> List[str]:
        """æ‰¹é‡æŸ¥è¯¢"""
        results = []
        
        for i, query in enumerate(queries):
            logger.info(f"Processing query {i+1}/{len(queries)}")
            result = self.simple_query(query, provider=provider, model=model)
            results.append(result)
            
            # æ·»åŠ å°å»¶è¿Ÿé¿å…APIé™åˆ¶
            time.sleep(0.5)
        
        return results
    
    def similarity_check(self, text1: str, text2: str, 
                        provider: Optional[str] = None) -> float:
        """ä½¿ç”¨LLMè¿›è¡Œç›¸ä¼¼æ€§æ£€æŸ¥"""
        prompt = f"""
è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªæŠ€æœ¯æœ¯è¯­çš„ç›¸ä¼¼æ€§ï¼Œå¹¶ç»™å‡º0-1ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚

æœ¯è¯­1: {text1}
æœ¯è¯­2: {text2}

è¯·åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„æ•°å­—ï¼Œè¡¨ç¤ºç›¸ä¼¼åº¦ï¼ˆ1è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼Œ0è¡¨ç¤ºå®Œå…¨ä¸åŒï¼‰ã€‚
ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šï¼Œåªè¿”å›æ•°å­—ã€‚
        """.strip()
        
        try:
            response = self.simple_query(prompt, provider=provider)
            # å°è¯•æå–æ•°å­—
            import re
            numbers = re.findall(r'0\.\d+|1\.0|0|1', response)
            if numbers:
                return min(float(numbers[0]), 1.0)
            else:
                logger.warning(f"âš ï¸ Could not parse similarity score from: {response}")
                return 0.0
        except Exception as e:
            logger.error(f"âŒ Similarity check failed: {e}")
            return 0.0


if __name__ == "__main__":
    
    """æµ‹è¯•LLMå®¢æˆ·ç«¯åŠŸèƒ½"""
    client = LLMClient()
    
    print("ğŸ§ª Testing LLM Client")
    print("=" * 40)
    
    # æµ‹è¯•ç®€å•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯Large Language Modelï¼Ÿ",
        "GPT-4å’ŒClaudeæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "å‘é‡æ•°æ®åº“çš„ä¸»è¦ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    for query in test_queries:
        try:
            print(f"\nâ“ Query: {query}")
            response = client.simple_query(query)
            print(f"ğŸ¤– Response: {response[:200]}...")
        except Exception as e:
            print(f"âŒ Error: {e}")
    
    # æµ‹è¯•ç›¸ä¼¼æ€§æ£€æŸ¥
    print(f"\nğŸ” Testing similarity check:")
    similarity = client.similarity_check("LLM", "Large Language Model")
    print(f"'LLM' vs 'Large Language Model': {similarity}")