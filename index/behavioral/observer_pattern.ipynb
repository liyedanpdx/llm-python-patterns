{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observer Pattern for LLM Monitoring\n",
    "\n",
    "## Interactive demonstration of Observer Pattern in AI/LLM systems\n",
    "\n",
    "This notebook demonstrates how the Observer Pattern enables real-time monitoring, cost tracking, and performance optimization in LLM applications.\n",
    "\n",
    "### Key Learning Objectives:\n",
    "- Understand Observer Pattern implementation\n",
    "- Build production-ready LLM monitoring system\n",
    "- Implement real-time cost and performance tracking\n",
    "- Create extensible monitoring architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Observer Pattern Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Core Observer Pattern classes defined\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Any, Optional\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "# Event system for LLM operations\n",
    "class EventType(Enum):\n",
    "    \"\"\"Types of events in LLM operations\"\"\"\n",
    "    CALL_START = \"call_start\"\n",
    "    CALL_SUCCESS = \"call_success\"\n",
    "    CALL_ERROR = \"call_error\"\n",
    "    COST_ALERT = \"cost_alert\"\n",
    "    PERFORMANCE_ALERT = \"performance_alert\"\n",
    "\n",
    "@dataclass\n",
    "class LLMEvent:\n",
    "    \"\"\"Event data structure for LLM operations\"\"\"\n",
    "    event_type: EventType\n",
    "    timestamp: datetime\n",
    "    call_id: str\n",
    "    data: Dict[str, Any]\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.timestamp is None:\n",
    "            self.timestamp = datetime.now()\n",
    "\n",
    "# Observer interface\n",
    "class Observer(ABC):\n",
    "    \"\"\"Abstract observer interface\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def update(self, event: LLMEvent) -> None:\n",
    "        \"\"\"Handle event notification\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_name(self) -> str:\n",
    "        \"\"\"Get observer name for identification\"\"\"\n",
    "        return self.__class__.__name__\n",
    "\n",
    "print(\"✅ Core Observer Pattern classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Subject (Observable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client with Observer support created\n"
     ]
    }
   ],
   "source": [
    "class LLMClient:\n",
    "    \"\"\"LLM client with observer pattern support\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._observers: List[Observer] = []\n",
    "        self.call_counter = 0\n",
    "        \n",
    "        # Model pricing (per 1K tokens)\n",
    "        self.model_pricing = {\n",
    "            \"gpt-4\": 0.03,\n",
    "            \"gpt-3.5-turbo\": 0.002,\n",
    "            \"claude-3\": 0.025,\n",
    "            \"gemini-pro\": 0.001\n",
    "        }\n",
    "    \n",
    "    def attach(self, observer: Observer) -> None:\n",
    "        \"\"\"Attach an observer\"\"\"\n",
    "        if observer not in self._observers:\n",
    "            self._observers.append(observer)\n",
    "            print(f\"📎 Attached observer: {observer.get_name()}\")\n",
    "    \n",
    "    def detach(self, observer: Observer) -> None:\n",
    "        \"\"\"Detach an observer\"\"\"\n",
    "        if observer in self._observers:\n",
    "            self._observers.remove(observer)\n",
    "            print(f\"📎 Detached observer: {observer.get_name()}\")\n",
    "    \n",
    "    def notify(self, event: LLMEvent) -> None:\n",
    "        \"\"\"Notify all observers of an event\"\"\"\n",
    "        for observer in self._observers:\n",
    "            try:\n",
    "                observer.update(event)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Observer {observer.get_name()} failed: {e}\")\n",
    "    \n",
    "    def call_llm(self, prompt: str, model: str = \"gpt-3.5-turbo\", \n",
    "                 max_tokens: int = 100) -> str:\n",
    "        \"\"\"Simulate LLM API call with comprehensive monitoring\"\"\"\n",
    "        \n",
    "        self.call_counter += 1\n",
    "        call_id = f\"call_{self.call_counter:04d}\"\n",
    "        \n",
    "        # Notify call start\n",
    "        start_event = LLMEvent(\n",
    "            event_type=EventType.CALL_START,\n",
    "            timestamp=datetime.now(),\n",
    "            call_id=call_id,\n",
    "            data={\n",
    "                \"prompt\": prompt[:50] + \"...\" if len(prompt) > 50 else prompt,\n",
    "                \"model\": model,\n",
    "                \"max_tokens\": max_tokens\n",
    "            }\n",
    "        )\n",
    "        self.notify(start_event)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Simulate API call with realistic delays and occasional errors\n",
    "            response_time = self._simulate_api_call(model, max_tokens)\n",
    "            \n",
    "            # Simulate random errors (5% chance)\n",
    "            if random.random() < 0.05:\n",
    "                raise Exception(f\"API Error: Rate limit exceeded for {model}\")\n",
    "            \n",
    "            # Generate mock response\n",
    "            response = f\"Mock {model} response to: '{prompt[:30]}...'\"\n",
    "            actual_tokens = min(max_tokens, len(response.split()) * 1.3)  # Rough token estimate\n",
    "            cost = self._calculate_cost(model, actual_tokens)\n",
    "            \n",
    "            # Notify successful completion\n",
    "            success_event = LLMEvent(\n",
    "                event_type=EventType.CALL_SUCCESS,\n",
    "                timestamp=datetime.now(),\n",
    "                call_id=call_id,\n",
    "                data={\n",
    "                    \"response\": response,\n",
    "                    \"response_time\": response_time,\n",
    "                    \"cost\": cost,\n",
    "                    \"tokens_used\": actual_tokens,\n",
    "                    \"model\": model\n",
    "                }\n",
    "            )\n",
    "            self.notify(success_event)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Notify error\n",
    "            error_event = LLMEvent(\n",
    "                event_type=EventType.CALL_ERROR,\n",
    "                timestamp=datetime.now(),\n",
    "                call_id=call_id,\n",
    "                data={\n",
    "                    \"error\": str(e),\n",
    "                    \"response_time\": response_time,\n",
    "                    \"model\": model\n",
    "                }\n",
    "            )\n",
    "            self.notify(error_event)\n",
    "            \n",
    "            raise e\n",
    "    \n",
    "    def _simulate_api_call(self, model: str, tokens: int) -> float:\n",
    "        \"\"\"Simulate realistic API response times\"\"\"\n",
    "        base_delay = {\n",
    "            \"gpt-4\": 0.8,\n",
    "            \"gpt-3.5-turbo\": 0.3,\n",
    "            \"claude-3\": 0.5,\n",
    "            \"gemini-pro\": 0.4\n",
    "        }.get(model, 0.5)\n",
    "        \n",
    "        # Add token-based delay and some randomness\n",
    "        delay = base_delay + (tokens / 1000) + random.uniform(0, 0.3)\n",
    "        time.sleep(delay)\n",
    "        return delay\n",
    "    \n",
    "    def _calculate_cost(self, model: str, tokens: float) -> float:\n",
    "        \"\"\"Calculate API call cost\"\"\"\n",
    "        price_per_1k = self.model_pricing.get(model, 0.01)\n",
    "        return (tokens / 1000) * price_per_1k\n",
    "\n",
    "print(\"✅ LLM Client with Observer support created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Concrete Observers for LLM Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Observer implementations created\n"
     ]
    }
   ],
   "source": [
    "class CostMonitor(Observer):\n",
    "    \"\"\"Monitor and track API costs with budget controls\"\"\"\n",
    "    \n",
    "    def __init__(self, daily_budget: float = 50.0, alert_threshold: float = 0.8):\n",
    "        self.daily_budget = daily_budget\n",
    "        self.alert_threshold = alert_threshold\n",
    "        self.daily_cost = 0.0\n",
    "        self.total_cost = 0.0\n",
    "        self.cost_by_model = {}\n",
    "        self.call_costs = []\n",
    "    \n",
    "    def update(self, event: LLMEvent) -> None:\n",
    "        if event.event_type == EventType.CALL_SUCCESS:\n",
    "            cost = event.data.get('cost', 0)\n",
    "            model = event.data.get('model')\n",
    "            \n",
    "            # Update cost tracking\n",
    "            self.daily_cost += cost\n",
    "            self.total_cost += cost\n",
    "            self.call_costs.append(cost)\n",
    "            \n",
    "            # Track by model\n",
    "            if model not in self.cost_by_model:\n",
    "                self.cost_by_model[model] = 0\n",
    "            self.cost_by_model[model] += cost\n",
    "            \n",
    "            print(f\"💰 Cost: +${cost:.4f} | Daily: ${self.daily_cost:.3f}/{self.daily_budget}\")\n",
    "            \n",
    "            # Budget alert\n",
    "            if self.daily_cost > self.daily_budget * self.alert_threshold:\n",
    "                utilization = (self.daily_cost / self.daily_budget) * 100\n",
    "                print(f\"🚨 BUDGET ALERT: {utilization:.1f}% of daily budget used!\")\n",
    "    \n",
    "    def get_cost_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive cost analysis\"\"\"\n",
    "        avg_cost = sum(self.call_costs) / len(self.call_costs) if self.call_costs else 0\n",
    "        \n",
    "        return {\n",
    "            \"daily_cost\": self.daily_cost,\n",
    "            \"daily_budget\": self.daily_budget,\n",
    "            \"budget_utilization_percent\": (self.daily_cost / self.daily_budget) * 100,\n",
    "            \"total_cost\": self.total_cost,\n",
    "            \"average_cost_per_call\": avg_cost,\n",
    "            \"cost_by_model\": self.cost_by_model,\n",
    "            \"total_calls\": len(self.call_costs)\n",
    "        }\n",
    "\n",
    "class PerformanceMonitor(Observer):\n",
    "    \"\"\"Monitor response times and performance metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, slow_threshold: float = 2.0):\n",
    "        self.slow_threshold = slow_threshold\n",
    "        self.response_times = []\n",
    "        self.model_performance = {}\n",
    "        self.slow_calls = 0\n",
    "    \n",
    "    def update(self, event: LLMEvent) -> None:\n",
    "        if event.event_type in [EventType.CALL_SUCCESS, EventType.CALL_ERROR]:\n",
    "            response_time = event.data.get('response_time', 0)\n",
    "            model = event.data.get('model')\n",
    "            \n",
    "            self.response_times.append(response_time)\n",
    "            \n",
    "            # Track by model\n",
    "            if model not in self.model_performance:\n",
    "                self.model_performance[model] = []\n",
    "            self.model_performance[model].append(response_time)\n",
    "            \n",
    "            # Performance feedback\n",
    "            if response_time > self.slow_threshold:\n",
    "                self.slow_calls += 1\n",
    "                print(f\"🐌 SLOW: {model} took {response_time:.2f}s (>{self.slow_threshold}s)\")\n",
    "            else:\n",
    "                print(f\"⚡ Fast: {model} responded in {response_time:.2f}s\")\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance analysis\"\"\"\n",
    "        if not self.response_times:\n",
    "            return {}\n",
    "        \n",
    "        avg_time = sum(self.response_times) / len(self.response_times)\n",
    "        \n",
    "        # Calculate model averages\n",
    "        model_averages = {}\n",
    "        for model, times in self.model_performance.items():\n",
    "            model_averages[model] = sum(times) / len(times)\n",
    "        \n",
    "        return {\n",
    "            \"average_response_time\": avg_time,\n",
    "            \"total_calls\": len(self.response_times),\n",
    "            \"slow_calls\": self.slow_calls,\n",
    "            \"slow_call_percentage\": (self.slow_calls / len(self.response_times)) * 100,\n",
    "            \"fastest_call\": min(self.response_times),\n",
    "            \"slowest_call\": max(self.response_times),\n",
    "            \"model_averages\": model_averages\n",
    "        }\n",
    "\n",
    "class ErrorTracker(Observer):\n",
    "    \"\"\"Track and categorize errors\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total_errors = 0\n",
    "        self.error_types = {}\n",
    "        self.recent_errors = []\n",
    "        self.error_rate_window = []\n",
    "    \n",
    "    def update(self, event: LLMEvent) -> None:\n",
    "        if event.event_type == EventType.CALL_START:\n",
    "            # Track calls for error rate calculation\n",
    "            self.error_rate_window.append(False)\n",
    "            \n",
    "        elif event.event_type == EventType.CALL_ERROR:\n",
    "            self.total_errors += 1\n",
    "            self.error_rate_window[-1] = True  # Mark last call as error\n",
    "            \n",
    "            error_msg = event.data.get('error', 'Unknown error')\n",
    "            error_type = error_msg.split(':')[0] if ':' in error_msg else 'Unknown'\n",
    "            \n",
    "            # Categorize errors\n",
    "            if error_type not in self.error_types:\n",
    "                self.error_types[error_type] = 0\n",
    "            self.error_types[error_type] += 1\n",
    "            \n",
    "            # Store recent errors (keep last 10)\n",
    "            self.recent_errors.append({\n",
    "                'call_id': event.call_id,\n",
    "                'error': error_msg,\n",
    "                'model': event.data.get('model'),\n",
    "                'timestamp': event.timestamp\n",
    "            })\n",
    "            \n",
    "            if len(self.recent_errors) > 10:\n",
    "                self.recent_errors.pop(0)\n",
    "            \n",
    "            print(f\"❌ ERROR: {event.call_id} - {error_msg}\")\n",
    "    \n",
    "    def get_error_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get error analysis\"\"\"\n",
    "        total_calls = len(self.error_rate_window)\n",
    "        error_rate = (self.total_errors / total_calls * 100) if total_calls > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_errors\": self.total_errors,\n",
    "            \"error_rate_percent\": error_rate,\n",
    "            \"error_types\": self.error_types,\n",
    "            \"recent_errors\": self.recent_errors[-3:],  # Last 3 errors\n",
    "            \"total_calls\": total_calls\n",
    "        }\n",
    "\n",
    "print(\"✅ Observer implementations created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Usage Analytics Observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Usage Analytics observer created\n"
     ]
    }
   ],
   "source": [
    "class UsageAnalytics(Observer):\n",
    "    \"\"\"Track usage patterns and trends\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total_calls = 0\n",
    "        self.successful_calls = 0\n",
    "        self.model_usage = {}\n",
    "        self.hourly_usage = {}\n",
    "        self.prompt_categories = {}\n",
    "    \n",
    "    def update(self, event: LLMEvent) -> None:\n",
    "        if event.event_type == EventType.CALL_START:\n",
    "            self.total_calls += 1\n",
    "            model = event.data.get('model')\n",
    "            \n",
    "            # Model usage tracking\n",
    "            if model not in self.model_usage:\n",
    "                self.model_usage[model] = 0\n",
    "            self.model_usage[model] += 1\n",
    "            \n",
    "            # Hourly usage pattern\n",
    "            hour = event.timestamp.hour\n",
    "            if hour not in self.hourly_usage:\n",
    "                self.hourly_usage[hour] = 0\n",
    "            self.hourly_usage[hour] += 1\n",
    "            \n",
    "            # Basic prompt categorization\n",
    "            prompt = event.data.get('prompt', '').lower()\n",
    "            category = self._categorize_prompt(prompt)\n",
    "            if category not in self.prompt_categories:\n",
    "                self.prompt_categories[category] = 0\n",
    "            self.prompt_categories[category] += 1\n",
    "            \n",
    "        elif event.event_type == EventType.CALL_SUCCESS:\n",
    "            self.successful_calls += 1\n",
    "    \n",
    "    def _categorize_prompt(self, prompt: str) -> str:\n",
    "        \"\"\"Simple prompt categorization\"\"\"\n",
    "        if any(word in prompt for word in ['explain', 'what is', 'define']):\n",
    "            return 'explanation'\n",
    "        elif any(word in prompt for word in ['write', 'generate', 'create']):\n",
    "            return 'generation'\n",
    "        elif any(word in prompt for word in ['analyze', 'review', 'examine']):\n",
    "            return 'analysis'\n",
    "        elif any(word in prompt for word in ['translate', 'convert']):\n",
    "            return 'translation'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    def get_usage_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get usage analytics\"\"\"\n",
    "        success_rate = (self.successful_calls / self.total_calls * 100) if self.total_calls > 0 else 0\n",
    "        \n",
    "        # Find most popular model and hour\n",
    "        popular_model = max(self.model_usage, key=self.model_usage.get) if self.model_usage else None\n",
    "        peak_hour = max(self.hourly_usage, key=self.hourly_usage.get) if self.hourly_usage else None\n",
    "        \n",
    "        return {\n",
    "            \"total_calls\": self.total_calls,\n",
    "            \"successful_calls\": self.successful_calls,\n",
    "            \"success_rate_percent\": success_rate,\n",
    "            \"model_usage\": self.model_usage,\n",
    "            \"most_popular_model\": popular_model,\n",
    "            \"peak_hour\": peak_hour,\n",
    "            \"prompt_categories\": self.prompt_categories\n",
    "        }\n",
    "\n",
    "print(\"✅ Usage Analytics observer created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demonstration: Building a Monitored LLM System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Setting up LLM Monitoring System\n",
      "==================================================\n",
      "📎 Attached observer: CostMonitor\n",
      "📎 Attached observer: PerformanceMonitor\n",
      "📎 Attached observer: ErrorTracker\n",
      "📎 Attached observer: UsageAnalytics\n",
      "\n",
      "📞 Starting LLM calls with real-time monitoring...\n",
      "\n",
      "\n",
      "[Call 1] Prompt: Explain machine learning in simple terms... | Model: gpt-3.5-turbo\n",
      "------------------------------------------------------------\n",
      "💰 Cost: +$0.0000 | Daily: $0.000/5.0\n",
      "⚡ Fast: gpt-3.5-turbo responded in 0.52s\n",
      "✅ Response: Mock gpt-3.5-turbo response to: 'Explain machine learning in...\n",
      "\n",
      "[Call 2] Prompt: Write a Python function to sort a list... | Model: gpt-4\n",
      "------------------------------------------------------------\n",
      "💰 Cost: +$0.0004 | Daily: $0.000/5.0\n",
      "⚡ Fast: gpt-4 responded in 1.03s\n",
      "✅ Response: Mock gpt-4 response to: 'Write a Python function to sor...'...\n",
      "\n",
      "[Call 3] Prompt: Analyze the sentiment of this text: 'I l... | Model: claude-3\n",
      "------------------------------------------------------------\n",
      "💰 Cost: +$0.0003 | Daily: $0.001/5.0\n",
      "⚡ Fast: claude-3 responded in 0.84s\n",
      "✅ Response: Mock claude-3 response to: 'Analyze the sentiment of this .....\n",
      "\n",
      "[Call 4] Prompt: Generate a creative story about space ex... | Model: gemini-pro\n",
      "------------------------------------------------------------\n",
      "💰 Cost: +$0.0000 | Daily: $0.001/5.0\n",
      "⚡ Fast: gemini-pro responded in 0.76s\n",
      "✅ Response: Mock gemini-pro response to: 'Generate a creative story abou...\n",
      "\n",
      "[Call 5] Prompt: Translate 'Hello world' to Spanish... | Model: gpt-3.5-turbo\n",
      "------------------------------------------------------------\n",
      "💰 Cost: +$0.0000 | Daily: $0.001/5.0\n",
      "⚡ Fast: gpt-3.5-turbo responded in 0.56s\n",
      "✅ Response: Mock gpt-3.5-turbo response to: 'Translate 'Hello world' to ...\n",
      "\n",
      "[Call 6] Prompt: What are the benefits of renewable energ... | Model: gpt-4\n",
      "------------------------------------------------------------\n",
      "💰 Cost: +$0.0004 | Daily: $0.001/5.0\n",
      "⚡ Fast: gpt-4 responded in 1.15s\n",
      "✅ Response: Mock gpt-4 response to: 'What are the benefits of renew...'...\n",
      "\n",
      "[Call 7] Prompt: Create a marketing email for a new produ... | Model: claude-3\n",
      "------------------------------------------------------------\n",
      "💰 Cost: +$0.0003 | Daily: $0.001/5.0\n",
      "⚡ Fast: claude-3 responded in 0.85s\n",
      "✅ Response: Mock claude-3 response to: 'Create a marketing email for a.....\n",
      "\n",
      "[Call 8] Prompt: Explain quantum computing concepts... | Model: gemini-pro\n",
      "------------------------------------------------------------\n",
      "💰 Cost: +$0.0000 | Daily: $0.001/5.0\n",
      "⚡ Fast: gemini-pro responded in 0.54s\n",
      "✅ Response: Mock gemini-pro response to: 'Explain quantum computing conc...\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_observer_pattern():\n",
    "    \"\"\"Comprehensive demonstration of Observer Pattern in LLM systems\"\"\"\n",
    "    \n",
    "    print(\"🚀 Setting up LLM Monitoring System\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create LLM client\n",
    "    llm = LLMClient()\n",
    "    \n",
    "    # Create observers\n",
    "    cost_monitor = CostMonitor(daily_budget=5.0, alert_threshold=0.7)\n",
    "    perf_monitor = PerformanceMonitor(slow_threshold=1.5)\n",
    "    error_tracker = ErrorTracker()\n",
    "    usage_analytics = UsageAnalytics()\n",
    "    \n",
    "    # Attach observers\n",
    "    llm.attach(cost_monitor)\n",
    "    llm.attach(perf_monitor)\n",
    "    llm.attach(error_tracker)\n",
    "    llm.attach(usage_analytics)\n",
    "    \n",
    "    print(\"\\n📞 Starting LLM calls with real-time monitoring...\\n\")\n",
    "    \n",
    "    # Test scenarios\n",
    "    test_scenarios = [\n",
    "        (\"Explain machine learning in simple terms\", \"gpt-3.5-turbo\", 80),\n",
    "        (\"Write a Python function to sort a list\", \"gpt-4\", 120),\n",
    "        (\"Analyze the sentiment of this text: 'I love AI!'\", \"claude-3\", 60),\n",
    "        (\"Generate a creative story about space exploration\", \"gemini-pro\", 200),\n",
    "        (\"Translate 'Hello world' to Spanish\", \"gpt-3.5-turbo\", 30),\n",
    "        (\"What are the benefits of renewable energy?\", \"gpt-4\", 150),\n",
    "        (\"Create a marketing email for a new product\", \"claude-3\", 180),\n",
    "        (\"Explain quantum computing concepts\", \"gemini-pro\", 100)\n",
    "    ]\n",
    "    \n",
    "    successful_calls = 0\n",
    "    \n",
    "    for i, (prompt, model, tokens) in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\n[Call {i}] Prompt: {prompt[:40]}... | Model: {model}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            response = llm.call_llm(prompt, model, tokens)\n",
    "            print(f\"✅ Response: {response[:60]}...\")\n",
    "            successful_calls += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed: {e}\")\n",
    "        \n",
    "        # Small delay between calls\n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    return llm, cost_monitor, perf_monitor, error_tracker, usage_analytics\n",
    "\n",
    "# Run demonstration\n",
    "llm_client, cost_mon, perf_mon, error_track, usage_stats = demonstrate_observer_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "📊 LLM MONITORING DASHBOARD\n",
      "======================================================================\n",
      "\n",
      "💰 COST ANALYSIS\n",
      "------------------------------\n",
      "Daily Cost: $0.001 / $5.00\n",
      "Budget Utilization: 0.0%\n",
      "Average Cost/Call: $0.0002\n",
      "Total Calls: 8\n",
      "\n",
      "Cost by Model:\n",
      "  gpt-3.5-turbo: $0.0000\n",
      "  gpt-4: $0.0008\n",
      "  claude-3: $0.0006\n",
      "  gemini-pro: $0.0000\n",
      "\n",
      "⚡ PERFORMANCE ANALYSIS\n",
      "------------------------------\n",
      "Average Response Time: 0.78s\n",
      "Slow Calls: 0/8 (0.0%)\n",
      "Fastest Call: 0.52s\n",
      "Slowest Call: 1.15s\n",
      "\n",
      "Model Performance:\n",
      "  gpt-3.5-turbo: 0.54s avg\n",
      "  gpt-4: 1.09s avg\n",
      "  claude-3: 0.84s avg\n",
      "  gemini-pro: 0.65s avg\n",
      "\n",
      "❌ ERROR ANALYSIS\n",
      "------------------------------\n",
      "Total Errors: 0\n",
      "Error Rate: 0.0%\n",
      "\n",
      "📈 USAGE ANALYTICS\n",
      "------------------------------\n",
      "Total Calls: 8\n",
      "Success Rate: 100.0%\n",
      "Most Popular Model: gpt-3.5-turbo\n",
      "Peak Hour: 22:00\n",
      "\n",
      "Model Usage Distribution:\n",
      "  gpt-3.5-turbo: 2 calls (25.0%)\n",
      "  gpt-4: 2 calls (25.0%)\n",
      "  claude-3: 2 calls (25.0%)\n",
      "  gemini-pro: 2 calls (25.0%)\n",
      "\n",
      "Prompt Categories:\n",
      "  explanation: 2 calls (25.0%)\n",
      "  generation: 3 calls (37.5%)\n",
      "  analysis: 1 calls (12.5%)\n",
      "  translation: 1 calls (12.5%)\n",
      "  other: 1 calls (12.5%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def display_monitoring_dashboard():\n",
    "    \"\"\"Display comprehensive monitoring dashboard\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"📊 LLM MONITORING DASHBOARD\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Cost Analysis\n",
    "    print(\"\\n💰 COST ANALYSIS\")\n",
    "    print(\"-\" * 30)\n",
    "    cost_data = cost_mon.get_cost_summary()\n",
    "    print(f\"Daily Cost: ${cost_data['daily_cost']:.3f} / ${cost_data['daily_budget']:.2f}\")\n",
    "    print(f\"Budget Utilization: {cost_data['budget_utilization_percent']:.1f}%\")\n",
    "    print(f\"Average Cost/Call: ${cost_data['average_cost_per_call']:.4f}\")\n",
    "    print(f\"Total Calls: {cost_data['total_calls']}\")\n",
    "    print(\"\\nCost by Model:\")\n",
    "    for model, cost in cost_data['cost_by_model'].items():\n",
    "        print(f\"  {model}: ${cost:.4f}\")\n",
    "    \n",
    "    # Performance Analysis\n",
    "    print(\"\\n⚡ PERFORMANCE ANALYSIS\")\n",
    "    print(\"-\" * 30)\n",
    "    perf_data = perf_mon.get_performance_summary()\n",
    "    if perf_data:\n",
    "        print(f\"Average Response Time: {perf_data['average_response_time']:.2f}s\")\n",
    "        print(f\"Slow Calls: {perf_data['slow_calls']}/{perf_data['total_calls']} ({perf_data['slow_call_percentage']:.1f}%)\")\n",
    "        print(f\"Fastest Call: {perf_data['fastest_call']:.2f}s\")\n",
    "        print(f\"Slowest Call: {perf_data['slowest_call']:.2f}s\")\n",
    "        print(\"\\nModel Performance:\")\n",
    "        for model, avg_time in perf_data['model_averages'].items():\n",
    "            print(f\"  {model}: {avg_time:.2f}s avg\")\n",
    "    \n",
    "    # Error Analysis\n",
    "    print(\"\\n❌ ERROR ANALYSIS\")\n",
    "    print(\"-\" * 30)\n",
    "    error_data = error_track.get_error_summary()\n",
    "    print(f\"Total Errors: {error_data['total_errors']}\")\n",
    "    print(f\"Error Rate: {error_data['error_rate_percent']:.1f}%\")\n",
    "    if error_data['error_types']:\n",
    "        print(\"Error Types:\")\n",
    "        for error_type, count in error_data['error_types'].items():\n",
    "            print(f\"  {error_type}: {count}\")\n",
    "    \n",
    "    # Usage Analytics\n",
    "    print(\"\\n📈 USAGE ANALYTICS\")\n",
    "    print(\"-\" * 30)\n",
    "    usage_data = usage_stats.get_usage_summary()\n",
    "    print(f\"Total Calls: {usage_data['total_calls']}\")\n",
    "    print(f\"Success Rate: {usage_data['success_rate_percent']:.1f}%\")\n",
    "    print(f\"Most Popular Model: {usage_data['most_popular_model']}\")\n",
    "    print(f\"Peak Hour: {usage_data['peak_hour']}:00\")\n",
    "    \n",
    "    print(\"\\nModel Usage Distribution:\")\n",
    "    for model, count in usage_data['model_usage'].items():\n",
    "        percentage = (count / usage_data['total_calls']) * 100\n",
    "        print(f\"  {model}: {count} calls ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nPrompt Categories:\")\n",
    "    for category, count in usage_data['prompt_categories'].items():\n",
    "        percentage = (count / usage_data['total_calls']) * 100\n",
    "        print(f\"  {category}: {count} calls ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Display the dashboard\n",
    "display_monitoring_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Observer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Testing Advanced Observer Features\n",
      "========================================\n",
      "📎 Attached observer: AlertManager\n",
      "📎 Attached observer: ConfigurableObserver(GPT-4 Tracker)\n",
      "📎 Attached observer: ConfigurableObserver(Expensive Call Tracker)\n",
      "\n",
      "🧪 Testing with additional calls...\n",
      "🎯 GPT-4 Event: call_start\n",
      "💰 Cost: +$0.0003 | Daily: $0.002/5.0\n",
      "⚡ Fast: gpt-4 responded in 1.30s\n",
      "🎯 GPT-4 Event: call_success\n",
      "💰 Cost: +$0.0000 | Daily: $0.002/5.0\n",
      "⚡ Fast: gpt-3.5-turbo responded in 0.44s\n",
      "🎯 GPT-4 Event: call_start\n",
      "💰 Cost: +$0.0003 | Daily: $0.002/5.0\n",
      "⚡ Fast: gpt-4 responded in 1.07s\n",
      "🎯 GPT-4 Event: call_success\n",
      "\n",
      "📊 Alert Summary: 0 alerts sent\n",
      "🎯 GPT-4 Events Processed: 4\n",
      "💸 Expensive Calls Tracked: 0\n",
      "\n",
      "✅ Advanced Observer features demonstrated!\n"
     ]
    }
   ],
   "source": [
    "class AlertManager(Observer):\n",
    "    \"\"\"Advanced alerting system with multiple notification channels\"\"\"\n",
    "    \n",
    "    def __init__(self, cost_threshold: float = 10.0, error_rate_threshold: float = 20.0):\n",
    "        self.cost_threshold = cost_threshold\n",
    "        self.error_rate_threshold = error_rate_threshold\n",
    "        self.alerts_sent = []\n",
    "        self.recent_errors = []\n",
    "        self.total_calls = 0\n",
    "        self.daily_cost = 0.0\n",
    "    \n",
    "    def update(self, event: LLMEvent) -> None:\n",
    "        if event.event_type == EventType.CALL_START:\n",
    "            self.total_calls += 1\n",
    "            \n",
    "        elif event.event_type == EventType.CALL_SUCCESS:\n",
    "            cost = event.data.get('cost', 0)\n",
    "            self.daily_cost += cost\n",
    "            \n",
    "            # Cost threshold alert\n",
    "            if self.daily_cost > self.cost_threshold:\n",
    "                self._send_alert(f\"🚨 COST ALERT: Daily spend ${self.daily_cost:.2f} exceeded threshold ${self.cost_threshold}\")\n",
    "                \n",
    "        elif event.event_type == EventType.CALL_ERROR:\n",
    "            self.recent_errors.append(event.timestamp)\n",
    "            \n",
    "            # Keep only last 10 errors for rate calculation\n",
    "            if len(self.recent_errors) > 10:\n",
    "                self.recent_errors.pop(0)\n",
    "            \n",
    "            # Error rate alert (if we have enough data)\n",
    "            if self.total_calls >= 5:\n",
    "                error_rate = (len(self.recent_errors) / min(self.total_calls, 10)) * 100\n",
    "                if error_rate > self.error_rate_threshold:\n",
    "                    self._send_alert(f\"🚨 ERROR RATE ALERT: {error_rate:.1f}% error rate detected\")\n",
    "    \n",
    "    def _send_alert(self, message: str) -> None:\n",
    "        \"\"\"Send alert (in real system: email, Slack, SMS, etc.)\"\"\"\n",
    "        alert = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'message': message\n",
    "        }\n",
    "        self.alerts_sent.append(alert)\n",
    "        print(f\"🔔 ALERT: {message}\")\n",
    "    \n",
    "    def get_alerts_summary(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'total_alerts': len(self.alerts_sent),\n",
    "            'recent_alerts': self.alerts_sent[-3:] if self.alerts_sent else []\n",
    "        }\n",
    "\n",
    "class ConfigurableObserver(Observer):\n",
    "    \"\"\"Observer with configurable filtering and processing\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, event_filter=None, processor=None):\n",
    "        self.name = name\n",
    "        self.event_filter = event_filter or (lambda event: True)\n",
    "        self.processor = processor or (lambda event: print(f\"[{self.name}] {event.event_type.value}\"))\n",
    "        self.processed_events = 0\n",
    "    \n",
    "    def update(self, event: LLMEvent) -> None:\n",
    "        if self.event_filter(event):\n",
    "            self.processor(event)\n",
    "            self.processed_events += 1\n",
    "    \n",
    "    def get_name(self) -> str:\n",
    "        return f\"ConfigurableObserver({self.name})\"\n",
    "\n",
    "# Demonstrate advanced observers\n",
    "print(\"🔧 Testing Advanced Observer Features\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create alert manager\n",
    "alert_manager = AlertManager(cost_threshold=0.1, error_rate_threshold=15.0)\n",
    "llm_client.attach(alert_manager)\n",
    "\n",
    "# Create configurable observers\n",
    "# Observer that only tracks GPT-4 calls\n",
    "gpt4_only = ConfigurableObserver(\n",
    "    \"GPT-4 Tracker\",\n",
    "    event_filter=lambda e: e.data.get('model') == 'gpt-4',\n",
    "    processor=lambda e: print(f\"🎯 GPT-4 Event: {e.event_type.value}\")\n",
    ")\n",
    "llm_client.attach(gpt4_only)\n",
    "\n",
    "# Observer that only tracks expensive calls\n",
    "expensive_calls = ConfigurableObserver(\n",
    "    \"Expensive Call Tracker\", \n",
    "    event_filter=lambda e: e.event_type == EventType.CALL_SUCCESS and e.data.get('cost', 0) > 0.005,\n",
    "    processor=lambda e: print(f\"💸 Expensive call: ${e.data.get('cost', 0):.4f}\")\n",
    ")\n",
    "llm_client.attach(expensive_calls)\n",
    "\n",
    "# Test with a few more calls\n",
    "print(\"\\n🧪 Testing with additional calls...\")\n",
    "test_calls = [\n",
    "    (\"Complex analysis task\", \"gpt-4\", 300),\n",
    "    (\"Simple question\", \"gpt-3.5-turbo\", 20),\n",
    "    (\"Another complex task\", \"gpt-4\", 250)\n",
    "]\n",
    "\n",
    "for prompt, model, tokens in test_calls:\n",
    "    try:\n",
    "        llm_client.call_llm(prompt, model, tokens)\n",
    "    except Exception as e:\n",
    "        pass  # Continue even if there are errors\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Show alert summary\n",
    "alert_summary = alert_manager.get_alerts_summary()\n",
    "print(f\"\\n📊 Alert Summary: {alert_summary['total_alerts']} alerts sent\")\n",
    "print(f\"🎯 GPT-4 Events Processed: {gpt4_only.processed_events}\")\n",
    "print(f\"💸 Expensive Calls Tracked: {expensive_calls.processed_events}\")\n",
    "\n",
    "print(\"\\n✅ Advanced Observer features demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Observer Pattern Benefits & Best Practices\n",
    "\n",
    "### Key Benefits Demonstrated:\n",
    "\n",
    "1. **Separation of Concerns**: Monitoring logic is completely separate from LLM logic\n",
    "2. **Extensibility**: Easy to add new types of monitoring without changing existing code\n",
    "3. **Real-time Feedback**: Immediate insights into cost, performance, and errors\n",
    "4. **Configurability**: Different observers for different needs and environments\n",
    "5. **Production Ready**: Handles errors gracefully, doesn't affect core functionality\n",
    "\n",
    "### Best Practices Applied:\n",
    "\n",
    "- **Error Isolation**: Observer failures don't crash the system\n",
    "- **Weak Coupling**: Observers don't depend on each other\n",
    "- **Event-Driven**: Clean event structure for extensibility\n",
    "- **Performance Conscious**: Minimal overhead in observer notifications\n",
    "- **Configurable**: Easy to enable/disable different monitoring aspects\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "This pattern is essential for:\n",
    "- **Production LLM APIs**: Cost control and performance monitoring\n",
    "- **Multi-Agent Systems**: Coordination and state synchronization\n",
    "- **Training Pipelines**: Progress tracking and metric collection\n",
    "- **AI Product Analytics**: Usage patterns and optimization insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integration with Real LLM APIs\n",
    "\n",
    "Here's how you would integrate this with actual LLM services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏭 Production Integration Example\n",
      "🔌 Production LLM Client initialized (using mock for demo)\n",
      "📎 Attached observer: CostMonitor\n",
      "📎 Attached observer: PerformanceMonitor\n",
      "💰 Cost: +$0.0045 | Daily: $0.004/100.0\n",
      "⚡ Fast: gpt-4 responded in 0.50s\n",
      "✅ Production response: Production gpt-4 response to: Explain the Observer Pattern i...\n",
      "\n",
      "🎯 Observer Pattern demonstration complete!\n",
      "The same monitoring system works seamlessly with production APIs.\n"
     ]
    }
   ],
   "source": [
    "# Example integration with OpenAI API\n",
    "class ProductionLLMClient(LLMClient):\n",
    "    \"\"\"Production LLM client with Observer pattern integration\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        super().__init__()\n",
    "        # self.openai_client = OpenAI(api_key=api_key)  # Uncomment for real usage\n",
    "        print(\"🔌 Production LLM Client initialized (using mock for demo)\")\n",
    "    \n",
    "    def call_llm(self, prompt: str, model: str = \"gpt-3.5-turbo\", \n",
    "                 max_tokens: int = 100) -> str:\n",
    "        \"\"\"Production LLM call with full observability\"\"\"\n",
    "        \n",
    "        call_id = f\"prod_call_{self.call_counter:04d}\"\n",
    "        self.call_counter += 1\n",
    "        \n",
    "        # Notify call start\n",
    "        start_event = LLMEvent(\n",
    "            event_type=EventType.CALL_START,\n",
    "            timestamp=datetime.now(),\n",
    "            call_id=call_id,\n",
    "            data={\n",
    "                \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
    "                \"model\": model,\n",
    "                \"max_tokens\": max_tokens\n",
    "            }\n",
    "        )\n",
    "        self.notify(start_event)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # In production, this would be:\n",
    "            # response = self.openai_client.chat.completions.create(\n",
    "            #     model=model,\n",
    "            #     messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            #     max_tokens=max_tokens\n",
    "            # )\n",
    "            \n",
    "            # For demo, simulate the call\n",
    "            time.sleep(0.5)  # Simulate API delay\n",
    "            response_text = f\"Production {model} response to: {prompt[:50]}...\"\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate actual costs based on real API pricing\n",
    "            actual_tokens = max_tokens  # In production: response.usage.total_tokens\n",
    "            cost = self._calculate_cost(model, actual_tokens)\n",
    "            \n",
    "            # Notify success\n",
    "            success_event = LLMEvent(\n",
    "                event_type=EventType.CALL_SUCCESS,\n",
    "                timestamp=datetime.now(),\n",
    "                call_id=call_id,\n",
    "                data={\n",
    "                    \"response\": response_text,\n",
    "                    \"response_time\": response_time,\n",
    "                    \"cost\": cost,\n",
    "                    \"tokens_used\": actual_tokens,\n",
    "                    \"model\": model\n",
    "                }\n",
    "            )\n",
    "            self.notify(success_event)\n",
    "            \n",
    "            return response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            error_event = LLMEvent(\n",
    "                event_type=EventType.CALL_ERROR,\n",
    "                timestamp=datetime.now(),\n",
    "                call_id=call_id,\n",
    "                data={\n",
    "                    \"error\": str(e),\n",
    "                    \"response_time\": response_time,\n",
    "                    \"model\": model\n",
    "                }\n",
    "            )\n",
    "            self.notify(error_event)\n",
    "            raise e\n",
    "\n",
    "# Example usage\n",
    "print(\"🏭 Production Integration Example\")\n",
    "prod_client = ProductionLLMClient()\n",
    "\n",
    "# Attach the same observers\n",
    "prod_client.attach(CostMonitor(daily_budget=100.0))\n",
    "prod_client.attach(PerformanceMonitor())\n",
    "\n",
    "# Test production client\n",
    "try:\n",
    "    response = prod_client.call_llm(\n",
    "        \"Explain the Observer Pattern in the context of distributed systems\",\n",
    "        \"gpt-4\",\n",
    "        150\n",
    "    )\n",
    "    print(f\"✅ Production response: {response[:60]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Production error: {e}\")\n",
    "\n",
    "print(\"\\n🎯 Observer Pattern demonstration complete!\")\n",
    "print(\"The same monitoring system works seamlessly with production APIs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
