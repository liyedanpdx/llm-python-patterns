{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factory Pattern in LLM Applications: Multi-Provider AI Client System\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use the **Factory Pattern** to create a flexible multi-provider AI client system. We'll build a system that can dynamically create different AI clients (OpenAI, Anthropic, Google, Local models) based on requirements.\n",
    "\n",
    "### Why Factory Pattern for Multi-LLM Systems?\n",
    "\n",
    "- **Dynamic Creation**: Choose AI providers at runtime based on cost, performance, or availability\n",
    "- **Decoupled Design**: Client code doesn't need to know specific implementation details\n",
    "- **Easy Extension**: Add new providers without modifying existing code\n",
    "- **Configuration-Driven**: Switch providers through configuration changes\n",
    "\n",
    "Let's start by exploring how traditional object creation becomes problematic with multiple AI providers, then see how the Factory Pattern elegantly solves these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Setting Up the Foundation\n",
    "\n",
    "First, let's define our base interfaces that all AI providers will implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Optional, Union\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Define provider types\n",
    "class ProviderType(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    GOOGLE = \"google\"\n",
    "    LOCAL = \"local\"\n",
    "    MOCK = \"mock\"  # For demonstration\n",
    "\n",
    "# Base AI Client Interface\n",
    "class AIClient(ABC):\n",
    "    \"\"\"Abstract base class for all AI clients\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate_text(self, prompt: str, **kwargs) -> Dict:\n",
    "        \"\"\"Generate text response from prompt\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"Get information about the model\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def estimate_cost(self, text: str) -> float:\n",
    "        \"\"\"Estimate cost for processing given text\"\"\"\n",
    "        pass\n",
    "\n",
    "print(\"‚úÖ Base interfaces defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è The Problem: Without Factory Pattern\n",
    "\n",
    "Let's see what happens when we create AI clients directly without using the Factory pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå PROBLEMATIC APPROACH: Direct instantiation everywhere\n",
    "\n",
    "class ProblematicAIService:\n",
    "    def __init__(self, provider_name: str):\n",
    "        # This approach has multiple problems:\n",
    "        # 1. Tight coupling\n",
    "        # 2. Hard to extend\n",
    "        # 3. Configuration scattered\n",
    "        # 4. Difficult to test\n",
    "        \n",
    "        if provider_name == \"openai\":\n",
    "            self.client = OpenAIClient(api_key=\"sk-...\", model=\"gpt-4\")\n",
    "        elif provider_name == \"anthropic\":\n",
    "            self.client = AnthropicClient(api_key=\"ant_...\", model=\"claude-3-opus\")\n",
    "        elif provider_name == \"google\":\n",
    "            self.client = GoogleClient(api_key=\"AI...\", model=\"gemini-pro\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown provider: {provider_name}\")\n",
    "    \n",
    "    def chat(self, message: str):\n",
    "        return self.client.generate_text(message)\n",
    "\n",
    "# Problems with this approach:\n",
    "print(\"‚ùå Problems with direct instantiation:\")\n",
    "print(\"1. Every place that creates clients needs to know all provider details\")\n",
    "print(\"2. Adding a new provider requires modifying this class\")\n",
    "print(\"3. Configuration is hardcoded and scattered\")\n",
    "print(\"4. Hard to unit test with mocks\")\n",
    "print(\"5. Violates Open/Closed Principle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ú® The Solution: Factory Pattern Implementation\n",
    "\n",
    "Now let's implement concrete AI clients that follow our interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete AI Client Implementations\n",
    "\n",
    "class OpenAIClient(AIClient):\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4\", **kwargs):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.cost_per_1k_tokens = 0.03\n",
    "        \n",
    "    def generate_text(self, prompt: str, **kwargs) -> Dict:\n",
    "        # Simulate OpenAI API call\n",
    "        response_time = random.uniform(1, 3)\n",
    "        time.sleep(0.1)  # Simulate network delay\n",
    "        \n",
    "        return {\n",
    "            \"text\": f\"[OpenAI {self.model}] Response to: {prompt[:50]}...\",\n",
    "            \"provider\": \"openai\",\n",
    "            \"model\": self.model,\n",
    "            \"tokens_used\": len(prompt.split()) + 20,\n",
    "            \"response_time\": response_time,\n",
    "            \"cost\": self.estimate_cost(prompt)\n",
    "        }\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        return {\n",
    "            \"provider\": \"OpenAI\",\n",
    "            \"model\": self.model,\n",
    "            \"max_tokens\": 4096,\n",
    "            \"cost_per_1k\": self.cost_per_1k_tokens,\n",
    "            \"strengths\": [\"General purpose\", \"Code generation\", \"Reasoning\"]\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self, text: str) -> float:\n",
    "        tokens = len(text.split()) * 1.3  # Rough token estimation\n",
    "        return (tokens / 1000) * self.cost_per_1k_tokens\n",
    "\n",
    "\n",
    "class AnthropicClient(AIClient):\n",
    "    def __init__(self, api_key: str, model: str = \"claude-3-opus-20240229\", **kwargs):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.cost_per_1k_tokens = 0.015\n",
    "        \n",
    "    def generate_text(self, prompt: str, **kwargs) -> Dict:\n",
    "        response_time = random.uniform(0.8, 2.5)\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        return {\n",
    "            \"text\": f\"[Anthropic {self.model}] Thoughtful response to: {prompt[:50]}...\",\n",
    "            \"provider\": \"anthropic\",\n",
    "            \"model\": self.model,\n",
    "            \"tokens_used\": len(prompt.split()) + 25,\n",
    "            \"response_time\": response_time,\n",
    "            \"cost\": self.estimate_cost(prompt)\n",
    "        }\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        return {\n",
    "            \"provider\": \"Anthropic\",\n",
    "            \"model\": self.model,\n",
    "            \"max_tokens\": 200000,\n",
    "            \"cost_per_1k\": self.cost_per_1k_tokens,\n",
    "            \"strengths\": [\"Safety\", \"Long context\", \"Analysis\"]\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self, text: str) -> float:\n",
    "        tokens = len(text.split()) * 1.3\n",
    "        return (tokens / 1000) * self.cost_per_1k_tokens\n",
    "\n",
    "\n",
    "class GoogleClient(AIClient):\n",
    "    def __init__(self, api_key: str, model: str = \"gemini-1.5-pro\", **kwargs):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.cost_per_1k_tokens = 0.0025\n",
    "        \n",
    "    def generate_text(self, prompt: str, **kwargs) -> Dict:\n",
    "        response_time = random.uniform(0.5, 2.0)\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        return {\n",
    "            \"text\": f\"[Google {self.model}] Efficient response to: {prompt[:50]}...\",\n",
    "            \"provider\": \"google\",\n",
    "            \"model\": self.model,\n",
    "            \"tokens_used\": len(prompt.split()) + 15,\n",
    "            \"response_time\": response_time,\n",
    "            \"cost\": self.estimate_cost(prompt)\n",
    "        }\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        return {\n",
    "            \"provider\": \"Google\",\n",
    "            \"model\": self.model,\n",
    "            \"max_tokens\": 1000000,\n",
    "            \"cost_per_1k\": self.cost_per_1k_tokens,\n",
    "            \"strengths\": [\"Fast\", \"Cost-effective\", \"Multimodal\"]\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self, text: str) -> float:\n",
    "        tokens = len(text.split()) * 1.3\n",
    "        return (tokens / 1000) * self.cost_per_1k_tokens\n",
    "\n",
    "\n",
    "class MockClient(AIClient):\n",
    "    \"\"\"Mock client for testing and development\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = \"mock-model\"\n",
    "        self.cost_per_1k_tokens = 0.0\n",
    "        \n",
    "    def generate_text(self, prompt: str, **kwargs) -> Dict:\n",
    "        return {\n",
    "            \"text\": f\"[MOCK] Instant response to: {prompt[:50]}...\",\n",
    "            \"provider\": \"mock\",\n",
    "            \"model\": self.model,\n",
    "            \"tokens_used\": len(prompt.split()) + 10,\n",
    "            \"response_time\": 0.01,\n",
    "            \"cost\": 0.0\n",
    "        }\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        return {\n",
    "            \"provider\": \"Mock\",\n",
    "            \"model\": self.model,\n",
    "            \"max_tokens\": 9999999,\n",
    "            \"cost_per_1k\": 0.0,\n",
    "            \"strengths\": [\"Testing\", \"Development\", \"Free\"]\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self, text: str) -> float:\n",
    "        return 0.0\n",
    "\n",
    "print(\"‚úÖ All concrete AI clients implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè≠ The Factory: Smart AI Client Creation\n",
    "\n",
    "Now for the star of the show - our Factory class that intelligently creates AI clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIClientFactory:\n",
    "    \"\"\"Factory for creating AI clients based on various criteria\"\"\"\n",
    "    \n",
    "    # Registry of available clients\n",
    "    _clients = {\n",
    "        ProviderType.OPENAI: OpenAIClient,\n",
    "        ProviderType.ANTHROPIC: AnthropicClient,\n",
    "        ProviderType.GOOGLE: GoogleClient,\n",
    "        ProviderType.MOCK: MockClient\n",
    "    }\n",
    "    \n",
    "    # Default configurations for each provider\n",
    "    _default_configs = {\n",
    "        ProviderType.OPENAI: {\n",
    "            \"api_key\": \"sk-demo-key\",\n",
    "            \"model\": \"gpt-4\",\n",
    "            \"temperature\": 0.7\n",
    "        },\n",
    "        ProviderType.ANTHROPIC: {\n",
    "            \"api_key\": \"ant-demo-key\", \n",
    "            \"model\": \"claude-3-opus-20240229\",\n",
    "            \"temperature\": 0.7\n",
    "        },\n",
    "        ProviderType.GOOGLE: {\n",
    "            \"api_key\": \"AI-demo-key\",\n",
    "            \"model\": \"gemini-1.5-pro\",\n",
    "            \"temperature\": 0.7\n",
    "        },\n",
    "        ProviderType.MOCK: {}\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def create_client(cls, provider_type: ProviderType, **kwargs) -> AIClient:\n",
    "        \"\"\"Create a client for the specified provider type\"\"\"\n",
    "        if provider_type not in cls._clients:\n",
    "            raise ValueError(f\"Unknown provider type: {provider_type}\")\n",
    "        \n",
    "        # Merge default config with provided kwargs\n",
    "        config = cls._default_configs[provider_type].copy()\n",
    "        config.update(kwargs)\n",
    "        \n",
    "        # Create and return the client\n",
    "        client_class = cls._clients[provider_type]\n",
    "        return client_class(**config)\n",
    "    \n",
    "    @classmethod\n",
    "    def create_client_by_name(cls, provider_name: str, **kwargs) -> AIClient:\n",
    "        \"\"\"Create a client by provider name string\"\"\"\n",
    "        try:\n",
    "            provider_type = ProviderType(provider_name.lower())\n",
    "            return cls.create_client(provider_type, **kwargs)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Unknown provider name: {provider_name}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def create_best_client_for_task(cls, task_requirements: Dict) -> AIClient:\n",
    "        \"\"\"Intelligently choose the best client based on task requirements\"\"\"\n",
    "        \n",
    "        # Cost-sensitive tasks\n",
    "        if task_requirements.get(\"cost_priority\", False):\n",
    "            print(\"üéØ Selecting cost-optimized provider: Google\")\n",
    "            return cls.create_client(ProviderType.GOOGLE)\n",
    "        \n",
    "        # Speed-critical tasks\n",
    "        if task_requirements.get(\"speed_priority\", False):\n",
    "            print(\"üéØ Selecting speed-optimized provider: Google\")\n",
    "            return cls.create_client(ProviderType.GOOGLE)\n",
    "        \n",
    "        # Safety-critical tasks\n",
    "        if task_requirements.get(\"safety_priority\", False):\n",
    "            print(\"üéØ Selecting safety-focused provider: Anthropic\")\n",
    "            return cls.create_client(ProviderType.ANTHROPIC)\n",
    "        \n",
    "        # Long context tasks\n",
    "        if task_requirements.get(\"long_context\", False):\n",
    "            print(\"üéØ Selecting long-context provider: Anthropic\")\n",
    "            return cls.create_client(ProviderType.ANTHROPIC)\n",
    "        \n",
    "        # Development/testing\n",
    "        if task_requirements.get(\"development\", False):\n",
    "            print(\"üéØ Selecting development provider: Mock\")\n",
    "            return cls.create_client(ProviderType.MOCK)\n",
    "        \n",
    "        # Default: balanced choice\n",
    "        print(\"üéØ Selecting balanced provider: OpenAI\")\n",
    "        return cls.create_client(ProviderType.OPENAI)\n",
    "    \n",
    "    @classmethod\n",
    "    def register_client(cls, provider_type: ProviderType, client_class, default_config: Dict):\n",
    "        \"\"\"Register a new client type (for extensibility)\"\"\"\n",
    "        cls._clients[provider_type] = client_class\n",
    "        cls._default_configs[provider_type] = default_config\n",
    "        print(f\"‚úÖ Registered new client: {provider_type}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_available_providers(cls) -> List[str]:\n",
    "        \"\"\"Get list of available providers\"\"\"\n",
    "        return [provider.value for provider in cls._clients.keys()]\n",
    "    \n",
    "    @classmethod\n",
    "    def compare_providers(cls, prompt: str) -> Dict:\n",
    "        \"\"\"Compare all providers for a given prompt\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for provider_type in cls._clients.keys():\n",
    "            client = cls.create_client(provider_type)\n",
    "            info = client.get_model_info()\n",
    "            cost = client.estimate_cost(prompt)\n",
    "            \n",
    "            results[provider_type.value] = {\n",
    "                \"model_info\": info,\n",
    "                \"estimated_cost\": cost\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"üè≠ AI Client Factory created!\")\n",
    "print(f\"üìã Available providers: {AIClientFactory.get_available_providers()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Demo 1: Basic Factory Usage\n",
    "\n",
    "Let's see the Factory Pattern in action with basic client creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DEMO 1: Basic Factory Usage ===\")\n",
    "\n",
    "# Creating clients using different methods\n",
    "print(\"\\n1. Creating clients by ProviderType enum:\")\n",
    "openai_client = AIClientFactory.create_client(ProviderType.OPENAI)\n",
    "print(f\"   ‚úÖ Created: {openai_client.get_model_info()['provider']} client\")\n",
    "\n",
    "print(\"\\n2. Creating clients by name string:\")\n",
    "anthropic_client = AIClientFactory.create_client_by_name(\"anthropic\")\n",
    "print(f\"   ‚úÖ Created: {anthropic_client.get_model_info()['provider']} client\")\n",
    "\n",
    "print(\"\\n3. Creating clients with custom configuration:\")\n",
    "custom_client = AIClientFactory.create_client(\n",
    "    ProviderType.GOOGLE, \n",
    "    model=\"gemini-1.5-flash\",  # Override default model\n",
    "    temperature=0.2\n",
    ")\n",
    "print(f\"   ‚úÖ Created: {custom_client.get_model_info()['provider']} client with custom config\")\n",
    "\n",
    "# Test all clients with the same prompt\n",
    "test_prompt = \"Explain quantum computing in simple terms\"\n",
    "clients = [openai_client, anthropic_client, custom_client]\n",
    "\n",
    "print(f\"\\n4. Testing all clients with prompt: '{test_prompt}'\")\n",
    "for i, client in enumerate(clients, 1):\n",
    "    response = client.generate_text(test_prompt)\n",
    "    print(f\"   Client {i} ({response['provider']}): {response['text'][:60]}...\")\n",
    "    print(f\"             Cost: ${response['cost']:.4f}, Time: {response['response_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Demo 2: Intelligent Provider Selection\n",
    "\n",
    "The real power of our Factory: automatically choosing the best provider based on task requirements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DEMO 2: Intelligent Provider Selection ===\")\n",
    "\n",
    "# Different task scenarios\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"Budget-Conscious Startup\",\n",
    "        \"requirements\": {\"cost_priority\": True},\n",
    "        \"prompt\": \"Write a product description for our app\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Real-Time Chat Application\", \n",
    "        \"requirements\": {\"speed_priority\": True},\n",
    "        \"prompt\": \"Generate a quick response to user greeting\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Healthcare AI Assistant\",\n",
    "        \"requirements\": {\"safety_priority\": True},\n",
    "        \"prompt\": \"Provide general wellness information\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Legal Document Analyzer\",\n",
    "        \"requirements\": {\"long_context\": True},\n",
    "        \"prompt\": \"Analyze this complex legal document\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Development Environment\",\n",
    "        \"requirements\": {\"development\": True},\n",
    "        \"prompt\": \"Test prompt for development\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"\\nüìã Scenario: {scenario['name']}\")\n",
    "    print(f\"   Requirements: {scenario['requirements']}\")\n",
    "    \n",
    "    # Factory automatically selects best provider\n",
    "    client = AIClientFactory.create_best_client_for_task(scenario['requirements'])\n",
    "    \n",
    "    # Generate response\n",
    "    response = client.generate_text(scenario['prompt'])\n",
    "    model_info = client.get_model_info()\n",
    "    \n",
    "    print(f\"   Selected: {model_info['provider']} ({model_info['model']})\")\n",
    "    print(f\"   Strengths: {', '.join(model_info['strengths'])}\")\n",
    "    print(f\"   Response: {response['text'][:70]}...\")\n",
    "    print(f\"   Cost: ${response['cost']:.4f}, Time: {response['response_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Demo 3: Provider Comparison\n",
    "\n",
    "Let's compare all providers side-by-side for the same task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DEMO 3: Provider Comparison ===\")\n",
    "\n",
    "comparison_prompt = \"Write a Python function to calculate fibonacci numbers\"\n",
    "print(f\"\\nüîç Comparing all providers for: '{comparison_prompt}'\")\n",
    "\n",
    "# Get comparison data\n",
    "comparison = AIClientFactory.compare_providers(comparison_prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'Provider':<12} {'Model':<25} {'Cost/1K':<10} {'Est.Cost':<10} {'Strengths':<25}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for provider_name, data in comparison.items():\n",
    "    info = data['model_info']\n",
    "    cost = data['estimated_cost']\n",
    "    \n",
    "    strengths = ', '.join(info['strengths'][:2])  # Show first 2 strengths\n",
    "    \n",
    "    print(f\"{info['provider']:<12} {info['model']:<25} ${info['cost_per_1k']:<9.4f} ${cost:<9.4f} {strengths:<25}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Now let's actually test each provider\n",
    "print(\"\\nüöÄ Live Response Comparison:\")\n",
    "responses = []\n",
    "\n",
    "for provider_name in comparison.keys():\n",
    "    if provider_name != \"mock\":  # Skip mock for more realistic comparison\n",
    "        client = AIClientFactory.create_client_by_name(provider_name)\n",
    "        response = client.generate_text(comparison_prompt)\n",
    "        responses.append(response)\n",
    "        \n",
    "        print(f\"\\nü§ñ {response['provider'].upper()}:\")\n",
    "        print(f\"   Response: {response['text']}\")\n",
    "        print(f\"   Metrics: {response['tokens_used']} tokens, ${response['cost']:.4f}, {response['response_time']:.2f}s\")\n",
    "\n",
    "# Summary statistics\n",
    "if responses:\n",
    "    avg_cost = sum(r['cost'] for r in responses) / len(responses)\n",
    "    avg_time = sum(r['response_time'] for r in responses) / len(responses)\n",
    "    fastest = min(responses, key=lambda r: r['response_time'])\n",
    "    cheapest = min(responses, key=lambda r: r['cost'])\n",
    "    \n",
    "    print(f\"\\nüìà Summary Statistics:\")\n",
    "    print(f\"   Average cost: ${avg_cost:.4f}\")\n",
    "    print(f\"   Average time: {avg_time:.2f}s\")\n",
    "    print(f\"   Fastest: {fastest['provider']} ({fastest['response_time']:.2f}s)\")\n",
    "    print(f\"   Cheapest: {cheapest['provider']} (${cheapest['cost']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Demo 4: Extensibility - Adding New Providers\n",
    "\n",
    "One of the Factory Pattern's greatest strengths is extensibility. Let's add a new provider without modifying existing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DEMO 4: Adding New Providers ===\")\n",
    "\n",
    "# Let's create a new provider - a local model\n",
    "class LocalLlamaClient(AIClient):\n",
    "    \"\"\"Example local model client (like Ollama)\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"llama2-7b\", **kwargs):\n",
    "        self.model = model\n",
    "        self.cost_per_1k_tokens = 0.0  # Local models are \"free\" after setup\n",
    "        \n",
    "    def generate_text(self, prompt: str, **kwargs) -> Dict:\n",
    "        # Local models are slower but free\n",
    "        response_time = random.uniform(3, 8)  # Slower than cloud APIs\n",
    "        time.sleep(0.2)  # Simulate processing time\n",
    "        \n",
    "        return {\n",
    "            \"text\": f\"[Local {self.model}] Private response to: {prompt[:50]}...\",\n",
    "            \"provider\": \"local_llama\",\n",
    "            \"model\": self.model,\n",
    "            \"tokens_used\": len(prompt.split()) + 30,\n",
    "            \"response_time\": response_time,\n",
    "            \"cost\": 0.0\n",
    "        }\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        return {\n",
    "            \"provider\": \"Local Llama\",\n",
    "            \"model\": self.model,\n",
    "            \"max_tokens\": 4096,\n",
    "            \"cost_per_1k\": 0.0,\n",
    "            \"strengths\": [\"Private\", \"Free\", \"Customizable\"]\n",
    "        }\n",
    "    \n",
    "    def estimate_cost(self, text: str) -> float:\n",
    "        return 0.0\n",
    "\n",
    "# Register the new provider with our factory\n",
    "class ExtendedProviderType(Enum):\n",
    "    LOCAL_LLAMA = \"local_llama\"\n",
    "\n",
    "# Register the new client type\n",
    "AIClientFactory.register_client(\n",
    "    ExtendedProviderType.LOCAL_LLAMA,\n",
    "    LocalLlamaClient,\n",
    "    {\"model\": \"llama2-13b\"}\n",
    ")\n",
    "\n",
    "print(\"\\nüÜï New provider registered! Testing it out...\")\n",
    "\n",
    "# Create and test the new client\n",
    "local_client = AIClientFactory.create_client(ExtendedProviderType.LOCAL_LLAMA)\n",
    "response = local_client.generate_text(\"What are the benefits of running models locally?\")\n",
    "\n",
    "print(f\"\\nü§ñ New Provider Test:\")\n",
    "print(f\"   Provider: {response['provider']}\")\n",
    "print(f\"   Response: {response['text']}\")\n",
    "print(f\"   Cost: ${response['cost']:.4f} (Free!)\")\n",
    "print(f\"   Time: {response['response_time']:.2f}s (Slower but private)\")\n",
    "\n",
    "# Show updated provider list\n",
    "print(f\"\\nüìã Updated available providers: {AIClientFactory.get_available_providers()}\")\n",
    "\n",
    "# The beauty of the Factory Pattern: no existing code needed to change!\n",
    "print(\"\\n‚ú® Key insight: We added a new provider without modifying ANY existing code!\")\n",
    "print(\"   This demonstrates the Open/Closed Principle - open for extension, closed for modification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Demo 5: Production-Ready Features\n",
    "\n",
    "Let's implement some production-ready features like failover, retry logic, and environment-based configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DEMO 5: Production-Ready Factory ===\")\n",
    "\n",
    "class ProductionAIClientFactory(AIClientFactory):\n",
    "    \"\"\"Enhanced factory with production features\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def create_resilient_client(cls, primary_provider: ProviderType, \n",
    "                              fallback_providers: List[ProviderType] = None,\n",
    "                              max_retries: int = 3) -> 'ResilientAIClient':\n",
    "        \"\"\"Create a client with automatic failover and retry logic\"\"\"\n",
    "        \n",
    "        if fallback_providers is None:\n",
    "            fallback_providers = [ProviderType.MOCK]  # Always have mock as final fallback\n",
    "        \n",
    "        return ResilientAIClient(primary_provider, fallback_providers, max_retries, cls)\n",
    "    \n",
    "    @classmethod\n",
    "    def create_from_environment(cls, env: str = \"development\") -> AIClient:\n",
    "        \"\"\"Create client based on environment configuration\"\"\"\n",
    "        \n",
    "        env_configs = {\n",
    "            \"development\": {\n",
    "                \"provider\": ProviderType.MOCK,\n",
    "                \"reason\": \"Fast, free, no API keys needed\"\n",
    "            },\n",
    "            \"testing\": {\n",
    "                \"provider\": ProviderType.MOCK,\n",
    "                \"reason\": \"Consistent, deterministic responses\"\n",
    "            },\n",
    "            \"staging\": {\n",
    "                \"provider\": ProviderType.GOOGLE,\n",
    "                \"reason\": \"Cost-effective for pre-production testing\"\n",
    "            },\n",
    "            \"production\": {\n",
    "                \"provider\": ProviderType.OPENAI,\n",
    "                \"reason\": \"High quality, reliable service\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        config = env_configs.get(env, env_configs[\"development\"])\n",
    "        print(f\"üåç Environment: {env} -> {config['provider'].value} ({config['reason']})\")\n",
    "        \n",
    "        return cls.create_client(config[\"provider\"])\n",
    "\n",
    "\n",
    "class ResilientAIClient:\n",
    "    \"\"\"A resilient client that handles failures gracefully\"\"\"\n",
    "    \n",
    "    def __init__(self, primary_provider: ProviderType, \n",
    "                 fallback_providers: List[ProviderType],\n",
    "                 max_retries: int,\n",
    "                 factory_class):\n",
    "        self.primary_provider = primary_provider\n",
    "        self.fallback_providers = fallback_providers\n",
    "        self.max_retries = max_retries\n",
    "        self.factory = factory_class\n",
    "        \n",
    "        # Create all clients upfront\n",
    "        self.clients = {}\n",
    "        all_providers = [primary_provider] + fallback_providers\n",
    "        for provider in all_providers:\n",
    "            self.clients[provider] = self.factory.create_client(provider)\n",
    "    \n",
    "    def generate_text(self, prompt: str, **kwargs) -> Dict:\n",
    "        \"\"\"Generate text with automatic failover\"\"\"\n",
    "        \n",
    "        providers_to_try = [self.primary_provider] + self.fallback_providers\n",
    "        \n",
    "        for provider in providers_to_try:\n",
    "            client = self.clients[provider]\n",
    "            \n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    print(f\"üîÑ Attempting {provider.value} (attempt {attempt + 1}/{self.max_retries})\")\n",
    "                    \n",
    "                    # Simulate occasional failures for demo\n",
    "                    if provider != ProviderType.MOCK and random.random() < 0.3:  # 30% failure rate\n",
    "                        raise Exception(f\"Simulated {provider.value} API failure\")\n",
    "                    \n",
    "                    response = client.generate_text(prompt, **kwargs)\n",
    "                    response[\"resilient_info\"] = {\n",
    "                        \"provider_used\": provider.value,\n",
    "                        \"attempt_number\": attempt + 1,\n",
    "                        \"is_fallback\": provider != self.primary_provider\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"‚úÖ Success with {provider.value}!\")\n",
    "                    return response\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå {provider.value} failed (attempt {attempt + 1}): {str(e)}\")\n",
    "                    if attempt == self.max_retries - 1:  # Last attempt with this provider\n",
    "                        print(f\"üîÑ Moving to next provider...\")\n",
    "                        break\n",
    "                    time.sleep(0.1)  # Brief pause before retry\n",
    "        \n",
    "        raise Exception(\"All providers and retries exhausted\")\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        return self.clients[self.primary_provider].get_model_info()\n",
    "\n",
    "print(\"\\nüèóÔ∏è Production Factory Features:\")\n",
    "\n",
    "# Demo 1: Environment-based configuration\n",
    "print(\"\\n1. Environment-based client creation:\")\n",
    "for env in [\"development\", \"testing\", \"staging\", \"production\"]:\n",
    "    client = ProductionAIClientFactory.create_from_environment(env)\n",
    "    info = client.get_model_info()\n",
    "    print(f\"   {env:>11}: {info['provider']} - {info['model']}\")\n",
    "\n",
    "# Demo 2: Resilient client with failover\n",
    "print(\"\\n2. Resilient client with failover:\")\n",
    "resilient_client = ProductionAIClientFactory.create_resilient_client(\n",
    "    primary_provider=ProviderType.OPENAI,\n",
    "    fallback_providers=[ProviderType.GOOGLE, ProviderType.ANTHROPIC, ProviderType.MOCK],\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# Test resilient client\n",
    "print(\"\\nüîÑ Testing resilient client (with simulated failures):\")\n",
    "try:\n",
    "    response = resilient_client.generate_text(\"What is machine learning?\")\n",
    "    resilient_info = response[\"resilient_info\"]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Final result:\")\n",
    "    print(f\"   Provider used: {resilient_info['provider_used']}\")\n",
    "    print(f\"   Attempt number: {resilient_info['attempt_number']}\")\n",
    "    print(f\"   Was fallback: {resilient_info['is_fallback']}\")\n",
    "    print(f\"   Response: {response['text'][:60]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå All providers failed: {e}\")\n",
    "\n",
    "print(\"\\nüéØ Production Benefits:\")\n",
    "print(\"   ‚úÖ Environment-specific configurations\")\n",
    "print(\"   ‚úÖ Automatic failover and retry logic\")\n",
    "print(\"   ‚úÖ Graceful degradation\")\n",
    "print(\"   ‚úÖ High availability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Real-World Application: Multi-Provider Chat Service\n",
    "\n",
    "Let's build a complete chat service that uses our Factory Pattern to provide intelligent provider selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== REAL-WORLD APPLICATION: Multi-Provider Chat Service ===\")\n",
    "\n",
    "class IntelligentChatService:\n",
    "    \"\"\"A chat service that intelligently selects AI providers based on various factors\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.factory = ProductionAIClientFactory()\n",
    "        self.conversation_history = []\n",
    "        self.user_preferences = {}\n",
    "        \n",
    "    def set_user_preferences(self, user_id: str, preferences: Dict):\n",
    "        \"\"\"Set user-specific preferences\"\"\"\n",
    "        self.user_preferences[user_id] = preferences\n",
    "        print(f\"‚úÖ Preferences set for user {user_id}: {preferences}\")\n",
    "    \n",
    "    def analyze_message(self, message: str) -> Dict:\n",
    "        \"\"\"Analyze message to determine optimal provider\"\"\"\n",
    "        analysis = {\n",
    "            \"word_count\": len(message.split()),\n",
    "            \"estimated_tokens\": len(message.split()) * 1.3,\n",
    "            \"is_coding_related\": any(word in message.lower() for word in \n",
    "                                   [\"code\", \"function\", \"python\", \"javascript\", \"programming\", \"debug\"]),\n",
    "            \"is_creative\": any(word in message.lower() for word in \n",
    "                             [\"story\", \"poem\", \"creative\", \"imagine\", \"write\"]),\n",
    "            \"is_analytical\": any(word in message.lower() for word in \n",
    "                               [\"analyze\", \"compare\", \"evaluate\", \"research\", \"study\"]),\n",
    "            \"complexity\": \"high\" if len(message.split()) > 50 else \"medium\" if len(message.split()) > 20 else \"low\"\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def select_optimal_provider(self, message: str, user_id: str = None) -> AIClient:\n",
    "        \"\"\"Select the best provider based on message analysis and user preferences\"\"\"\n",
    "        \n",
    "        analysis = self.analyze_message(message)\n",
    "        user_prefs = self.user_preferences.get(user_id, {})\n",
    "        \n",
    "        print(f\"\\nüîç Message Analysis:\")\n",
    "        print(f\"   Word count: {analysis['word_count']}\")\n",
    "        print(f\"   Complexity: {analysis['complexity']}\")\n",
    "        print(f\"   Coding related: {analysis['is_coding_related']}\")\n",
    "        print(f\"   Creative: {analysis['is_creative']}\")\n",
    "        print(f\"   Analytical: {analysis['is_analytical']}\")\n",
    "        \n",
    "        requirements = {}\n",
    "        \n",
    "        # User preferences override\n",
    "        if user_prefs.get(\"cost_conscious\"):\n",
    "            requirements[\"cost_priority\"] = True\n",
    "            print(\"   üí∞ User prefers cost optimization\")\n",
    "        \n",
    "        if user_prefs.get(\"speed_important\"):\n",
    "            requirements[\"speed_priority\"] = True\n",
    "            print(\"   ‚ö° User prefers speed\")\n",
    "        \n",
    "        if user_prefs.get(\"privacy_focused\"):\n",
    "            # Would select local model in real implementation\n",
    "            print(\"   üîí User prefers privacy (would select local model)\")\n",
    "        \n",
    "        # Content-based selection\n",
    "        if analysis[\"is_coding_related\"]:\n",
    "            print(\"   üíª Detected coding task - selecting code-optimized provider\")\n",
    "            # OpenAI is often considered good for code\n",
    "        \n",
    "        if analysis[\"complexity\"] == \"high\":\n",
    "            requirements[\"quality_priority\"] = True\n",
    "            print(\"   üß† High complexity - selecting advanced provider\")\n",
    "        \n",
    "        if analysis[\"is_analytical\"]:\n",
    "            requirements[\"safety_priority\"] = True\n",
    "            print(\"   üìä Analytical task - selecting safety-focused provider\")\n",
    "        \n",
    "        return self.factory.create_best_client_for_task(requirements)\n",
    "    \n",
    "    def chat(self, message: str, user_id: str = \"default\") -> Dict:\n",
    "        \"\"\"Main chat method\"\"\"\n",
    "        \n",
    "        print(f\"\\nüí¨ User {user_id}: {message}\")\n",
    "        \n",
    "        # Select optimal provider\n",
    "        client = self.select_optimal_provider(message, user_id)\n",
    "        \n",
    "        # Generate response\n",
    "        response = client.generate_text(message)\n",
    "        model_info = client.get_model_info()\n",
    "        \n",
    "        # Store in conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"user_id\": user_id,\n",
    "            \"message\": message,\n",
    "            \"response\": response[\"text\"],\n",
    "            \"provider\": response[\"provider\"],\n",
    "            \"cost\": response[\"cost\"],\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "        \n",
    "        result = {\n",
    "            \"response\": response[\"text\"],\n",
    "            \"provider_used\": response[\"provider\"],\n",
    "            \"model\": model_info[\"model\"],\n",
    "            \"cost\": response[\"cost\"],\n",
    "            \"response_time\": response[\"response_time\"],\n",
    "            \"provider_strengths\": model_info[\"strengths\"]\n",
    "        }\n",
    "        \n",
    "        print(f\"ü§ñ {model_info['provider']}: {result['response']}\")\n",
    "        print(f\"   üìä Stats: ${result['cost']:.4f}, {result['response_time']:.2f}s, {result['provider_used']}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_conversation_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the conversation\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return {\"message\": \"No conversations yet\"}\n",
    "        \n",
    "        total_cost = sum(conv[\"cost\"] for conv in self.conversation_history)\n",
    "        provider_usage = {}\n",
    "        \n",
    "        for conv in self.conversation_history:\n",
    "            provider = conv[\"provider\"]\n",
    "            provider_usage[provider] = provider_usage.get(provider, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            \"total_messages\": len(self.conversation_history),\n",
    "            \"total_cost\": total_cost,\n",
    "            \"provider_usage\": provider_usage,\n",
    "            \"average_cost_per_message\": total_cost / len(self.conversation_history)\n",
    "        }\n",
    "\n",
    "# Create and test the chat service\n",
    "chat_service = IntelligentChatService()\n",
    "\n",
    "# Set up different user profiles\n",
    "chat_service.set_user_preferences(\"budget_user\", {\"cost_conscious\": True})\n",
    "chat_service.set_user_preferences(\"speed_user\", {\"speed_important\": True})\n",
    "chat_service.set_user_preferences(\"privacy_user\", {\"privacy_focused\": True})\n",
    "\n",
    "# Test different scenarios\n",
    "test_conversations = [\n",
    "    (\"budget_user\", \"What's the weather like today?\"),\n",
    "    (\"speed_user\", \"Quick question: what's 2+2?\"),\n",
    "    (\"privacy_user\", \"Help me write a personal email to my friend\"),\n",
    "    (\"default\", \"Write a Python function to sort a list of dictionaries by a specific key\"),\n",
    "    (\"default\", \"Analyze the pros and cons of renewable energy sources in detail, considering economic, environmental, and technological factors\"),\n",
    "    (\"default\", \"Write a creative short story about a robot learning to paint\")\n",
    "]\n",
    "\n",
    "for user_id, message in test_conversations:\n",
    "    chat_service.chat(message, user_id)\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Show conversation statistics\n",
    "stats = chat_service.get_conversation_stats()\n",
    "print(f\"\\nüìä CONVERSATION STATISTICS:\")\n",
    "print(f\"   Total messages: {stats['total_messages']}\")\n",
    "print(f\"   Total cost: ${stats['total_cost']:.4f}\")\n",
    "print(f\"   Average cost per message: ${stats['average_cost_per_message']:.4f}\")\n",
    "print(f\"   Provider usage: {stats['provider_usage']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways: Factory Pattern Benefits\n",
    "\n",
    "Let's summarize what we've learned about using the Factory Pattern in LLM applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== KEY TAKEAWAYS: Factory Pattern Benefits ===\")\n",
    "\n",
    "benefits = {\n",
    "    \"üéØ Dynamic Selection\": [\n",
    "        \"Choose AI providers based on runtime requirements\",\n",
    "        \"Optimize for cost, speed, quality, or specific capabilities\",\n",
    "        \"Adapt to changing conditions and user preferences\"\n",
    "    ],\n",
    "    \"üîß Extensibility\": [\n",
    "        \"Add new AI providers without modifying existing code\",\n",
    "        \"Plugin-like architecture for easy integration\",\n",
    "        \"Support for custom models and local deployments\"\n",
    "    ],\n",
    "    \"üõ°Ô∏è Robustness\": [\n",
    "        \"Automatic failover and retry mechanisms\",\n",
    "        \"Graceful degradation when providers are unavailable\",\n",
    "        \"Environment-specific configurations\"\n",
    "    ],\n",
    "    \"üíº Production Ready\": [\n",
    "        \"Cost tracking and optimization\",\n",
    "        \"Performance monitoring and comparison\",\n",
    "        \"User preference management\"\n",
    "    ],\n",
    "    \"üß™ Testing\": [\n",
    "        \"Easy mocking for unit tests\",\n",
    "        \"Consistent interfaces across providers\",\n",
    "        \"Isolated testing of individual components\"\n",
    "    ],\n",
    "    \"‚ö° Developer Experience\": [\n",
    "        \"Clean, readable code with clear abstractions\",\n",
    "        \"Configuration-driven behavior\",\n",
    "        \"Reduced boilerplate and duplication\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in benefits.items():\n",
    "    print(f\"\\n{category}\")\n",
    "    for item in items:\n",
    "        print(f\"   ‚Ä¢ {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ FACTORY PATTERN: The Foundation for Scalable AI Systems\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüåü When to Use Factory Pattern in LLM Applications:\")\n",
    "use_cases = [\n",
    "    \"Multiple AI providers or models to choose from\",\n",
    "    \"Different requirements for different use cases\", \n",
    "    \"Need for runtime provider selection\",\n",
    "    \"Environment-specific configurations\",\n",
    "    \"Cost optimization requirements\",\n",
    "    \"Failover and reliability needs\",\n",
    "    \"Future extensibility requirements\"\n",
    "]\n",
    "\n",
    "for i, use_case in enumerate(use_cases, 1):\n",
    "    print(f\"   {i}. {use_case}\")\n",
    "\n",
    "print(\"\\nüí° Pro Tip: The Factory Pattern is particularly powerful in LLM applications\")\n",
    "print(\"   because the AI landscape is rapidly evolving. What works best today\")\n",
    "print(\"   might be different tomorrow - the Factory Pattern keeps you adaptable!\")\n",
    "\n",
    "print(\"\\nüéâ Congratulations! You've mastered the Factory Pattern for LLM applications!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Next Steps and Integration\n",
    "\n",
    "### Integration with Other Patterns\n",
    "\n",
    "The Factory Pattern works excellently with other design patterns:\n",
    "\n",
    "- **Abstract Factory**: For creating families of related AI components (LLM + Embeddings + Tools)\n",
    "- **Strategy Pattern**: For different algorithms/approaches within each provider\n",
    "- **Builder Pattern**: For constructing complex prompts and configurations\n",
    "- **Chain of Responsibility**: For routing requests through different AI agents\n",
    "\n",
    "### Real-World Implementation Tips\n",
    "\n",
    "1. **Configuration Management**: Use environment variables or configuration files for API keys and settings\n",
    "2. **Error Handling**: Implement proper exception handling and logging\n",
    "3. **Rate Limiting**: Add rate limiting to prevent API quota exhaustion\n",
    "4. **Monitoring**: Track usage, costs, and performance metrics\n",
    "5. **Caching**: Implement response caching for repeated queries\n",
    "6. **Security**: Never hardcode API keys; use secure configuration management\n",
    "\n",
    "### Try It Yourself!\n",
    "\n",
    "Experiment with:\n",
    "- Adding your own custom AI provider\n",
    "- Implementing different selection strategies\n",
    "- Building more sophisticated failover logic\n",
    "- Creating domain-specific factories (e.g., for medical, legal, or financial applications)\n",
    "\n",
    "The Factory Pattern is your gateway to building flexible, maintainable, and scalable AI applications! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}