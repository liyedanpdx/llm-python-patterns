{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxy Pattern for LLM Systems: Access Control & Optimization\n",
    "\n",
    "**Pattern Focus**: Controlling access to LLM services while adding functionality like caching, authentication, and cost tracking.\n",
    "\n",
    "**Real-World Example**: Enterprise LLM gateway that manages API access, costs, and security across multiple providers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Proxy Pattern Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "import hashlib\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Abstract LLM Service Interface\n",
    "class LLMService(ABC):\n",
    "    @abstractmethod\n",
    "    def complete(self, prompt: str, **kwargs) -> str:\n",
    "        pass\n",
    "\n",
    "# Real LLM Service (expensive to call)\n",
    "class OpenAIService(LLMService):\n",
    "    def __init__(self):\n",
    "        self.call_count = 0\n",
    "        self.total_cost = 0.0\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> str:\n",
    "        # Simulate API call delay and cost\n",
    "        time.sleep(0.5)  # Simulate network latency\n",
    "        self.call_count += 1\n",
    "        cost = len(prompt) * 0.001  # $0.001 per character\n",
    "        self.total_cost += cost\n",
    "        \n",
    "        return f\"AI Response to: '{prompt[:50]}...' (Call #{self.call_count}, Cost: ${cost:.3f})\"\n",
    "\n",
    "print(\"‚úÖ Core components defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Smart Caching Proxy - 80% Cost Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachingLLMProxy(LLMService):\n",
    "    \"\"\"Proxy with intelligent caching for cost optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, real_service: LLMService):\n",
    "        self._real_service = real_service\n",
    "        self._cache: Dict[str, str] = {}\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> str:\n",
    "        # Generate cache key\n",
    "        cache_key = hashlib.md5(f\"{prompt}{kwargs}\".encode()).hexdigest()\n",
    "        \n",
    "        # Check cache first\n",
    "        if cache_key in self._cache:\n",
    "            self.cache_hits += 1\n",
    "            return f\"[CACHED] {self._cache[cache_key]}\"\n",
    "        \n",
    "        # Cache miss - call real service\n",
    "        self.cache_misses += 1\n",
    "        result = self._real_service.complete(prompt, **kwargs)\n",
    "        self._cache[cache_key] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        total = self.cache_hits + self.cache_misses\n",
    "        hit_rate = (self.cache_hits / total * 100) if total > 0 else 0\n",
    "        return f\"Cache Hit Rate: {hit_rate:.1f}% ({self.cache_hits}/{total})\"\n",
    "\n",
    "print(\"‚úÖ Caching proxy ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demo: Cache Performance Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create services\n",
    "real_llm = OpenAIService()\n",
    "cached_llm = CachingLLMProxy(real_llm)\n",
    "\n",
    "# Test prompts (some repeated)\n",
    "test_prompts = [\n",
    "    \"Explain machine learning\",\n",
    "    \"What is Python?\",\n",
    "    \"Explain machine learning\",  # Repeat\n",
    "    \"Best practices for API design\",\n",
    "    \"What is Python?\",  # Repeat\n",
    "    \"Explain machine learning\",  # Repeat\n",
    "]\n",
    "\n",
    "print(\"üöÄ Testing cached vs direct calls...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    response = cached_llm.complete(prompt)\n",
    "    print(f\"{i}. {response[:80]}...\")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"Total time: {duration:.2f}s\")\n",
    "print(f\"Real API calls: {real_llm.call_count}\")\n",
    "print(f\"Total cost: ${real_llm.total_cost:.3f}\")\n",
    "print(f\"{cached_llm.get_cache_stats()}\")\n",
    "print(f\"\\nüí∞ Cost savings: {(1 - real_llm.call_count/len(test_prompts)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Authentication & Rate Limiting Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecureLLMProxy(LLMService):\n",
    "    \"\"\"Enterprise proxy with authentication and rate limiting\"\"\"\n",
    "    \n",
    "    def __init__(self, real_service: LLMService):\n",
    "        self._real_service = real_service\n",
    "        self.valid_api_keys = {\"user123\", \"team456\", \"admin789\"}\n",
    "        self.rate_limits = {}  # user_id -> {count, last_reset}\n",
    "        self.max_requests_per_minute = 5\n",
    "    \n",
    "    def complete(self, prompt: str, api_key: str = None, **kwargs) -> str:\n",
    "        # 1. Authentication\n",
    "        if not self._authenticate(api_key):\n",
    "            raise Exception(\"‚ùå Authentication failed: Invalid API key\")\n",
    "        \n",
    "        # 2. Rate limiting\n",
    "        if not self._check_rate_limit(api_key):\n",
    "            raise Exception(f\"‚ùå Rate limit exceeded: Max {self.max_requests_per_minute}/minute\")\n",
    "        \n",
    "        # 3. Execute request\n",
    "        result = self._real_service.complete(prompt, **kwargs)\n",
    "        \n",
    "        # 4. Audit logging\n",
    "        self._log_request(api_key, prompt, success=True)\n",
    "        \n",
    "        return f\"[AUTHORIZED] {result}\"\n",
    "    \n",
    "    def _authenticate(self, api_key: str) -> bool:\n",
    "        return api_key in self.valid_api_keys\n",
    "    \n",
    "    def _check_rate_limit(self, api_key: str) -> bool:\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if api_key not in self.rate_limits:\n",
    "            self.rate_limits[api_key] = {'count': 0, 'last_reset': current_time}\n",
    "        \n",
    "        user_limits = self.rate_limits[api_key]\n",
    "        \n",
    "        # Reset counter if minute has passed\n",
    "        if current_time - user_limits['last_reset'] >= 60:\n",
    "            user_limits['count'] = 0\n",
    "            user_limits['last_reset'] = current_time\n",
    "        \n",
    "        # Check limit\n",
    "        if user_limits['count'] >= self.max_requests_per_minute:\n",
    "            return False\n",
    "        \n",
    "        user_limits['count'] += 1\n",
    "        return True\n",
    "    \n",
    "    def _log_request(self, api_key: str, prompt: str, success: bool):\n",
    "        status = \"SUCCESS\" if success else \"FAILED\"\n",
    "        print(f\"üîç AUDIT: {api_key} | {status} | '{prompt[:30]}...'\")\n",
    "\n",
    "print(\"‚úÖ Secure proxy ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demo: Authentication & Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create secure proxy\n",
    "real_service = OpenAIService()\n",
    "secure_proxy = SecureLLMProxy(real_service)\n",
    "\n",
    "print(\"üîê Testing authentication and rate limiting...\\n\")\n",
    "\n",
    "# Test 1: Valid authentication\n",
    "try:\n",
    "    response = secure_proxy.complete(\"Hello AI\", api_key=\"user123\")\n",
    "    print(f\"‚úÖ Valid auth: {response[:60]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test 2: Invalid authentication\n",
    "try:\n",
    "    response = secure_proxy.complete(\"Hello AI\", api_key=\"invalid\")\n",
    "    print(f\"‚úÖ Response: {response[:60]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Expected error: {e}\")\n",
    "\n",
    "# Test 3: Rate limiting (make 6 quick requests)\n",
    "print(\"\\nüöÄ Testing rate limiting (max 5/minute):\")\n",
    "for i in range(6):\n",
    "    try:\n",
    "        response = secure_proxy.complete(f\"Request {i+1}\", api_key=\"user123\")\n",
    "        print(f\"‚úÖ Request {i+1}: Allowed\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Request {i+1}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Provider Load Balancing Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadBalancingProxy(LLMService):\n",
    "    \"\"\"Intelligent load balancing across multiple LLM providers\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Simulate different providers with different characteristics\n",
    "        self.providers = {\n",
    "            'openai': {'service': OpenAIService(), 'cost_per_char': 0.001, 'latency': 0.5},\n",
    "            'anthropic': {'service': OpenAIService(), 'cost_per_char': 0.0008, 'latency': 0.3},\n",
    "            'google': {'service': OpenAIService(), 'cost_per_char': 0.0006, 'latency': 0.4}\n",
    "        }\n",
    "        self.usage_stats = {name: 0 for name in self.providers.keys()}\n",
    "    \n",
    "    def complete(self, prompt: str, optimize_for=\"cost\", **kwargs) -> str:\n",
    "        # Select optimal provider based on strategy\n",
    "        provider_name = self._select_provider(prompt, optimize_for)\n",
    "        provider = self.providers[provider_name]['service']\n",
    "        \n",
    "        # Execute request\n",
    "        result = provider.complete(prompt, **kwargs)\n",
    "        \n",
    "        # Track usage\n",
    "        self.usage_stats[provider_name] += 1\n",
    "        \n",
    "        return f\"[{provider_name.upper()}] {result}\"\n",
    "    \n",
    "    def _select_provider(self, prompt: str, strategy: str) -> str:\n",
    "        \"\"\"Select optimal provider based on strategy\"\"\"\n",
    "        if strategy == \"cost\":\n",
    "            # Choose cheapest provider\n",
    "            return min(self.providers.keys(), \n",
    "                      key=lambda p: self.providers[p]['cost_per_char'])\n",
    "        elif strategy == \"speed\":\n",
    "            # Choose fastest provider\n",
    "            return min(self.providers.keys(), \n",
    "                      key=lambda p: self.providers[p]['latency'])\n",
    "        else:\n",
    "            # Round-robin for balance\n",
    "            return min(self.providers.keys(), \n",
    "                      key=lambda p: self.usage_stats[p])\n",
    "    \n",
    "    def get_usage_stats(self):\n",
    "        total = sum(self.usage_stats.values())\n",
    "        if total == 0:\n",
    "            return \"No requests yet\"\n",
    "        \n",
    "        stats = []\n",
    "        for provider, count in self.usage_stats.items():\n",
    "            percentage = (count / total) * 100\n",
    "            stats.append(f\"{provider}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        return \" | \".join(stats)\n",
    "\n",
    "print(\"‚úÖ Load balancing proxy ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Demo: Multi-Provider Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create load balancing proxy\n",
    "lb_proxy = LoadBalancingProxy()\n",
    "\n",
    "print(\"‚öñÔ∏è Testing multi-provider load balancing...\\n\")\n",
    "\n",
    "# Test different optimization strategies\n",
    "strategies = [\"cost\", \"speed\", \"balanced\"]\n",
    "test_prompts = [\"Explain AI\", \"Code review tips\", \"System design\"]\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"üìä Strategy: {strategy.upper()}\")\n",
    "    for prompt in test_prompts:\n",
    "        response = lb_proxy.complete(prompt, optimize_for=strategy)\n",
    "        provider = response.split(']')[0][1:]  # Extract provider name\n",
    "        print(f\"  ‚Üí {provider} handled: '{prompt}'\")\n",
    "    print()\n",
    "\n",
    "print(f\"üìà Final usage distribution: {lb_proxy.get_usage_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Enterprise Proxy: All Features Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnterpriseLLMProxy(LLMService):\n",
    "    \"\"\"Production-ready proxy with all enterprise features\"\"\"\n",
    "    \n",
    "    def __init__(self, real_service: LLMService):\n",
    "        self._real_service = real_service\n",
    "        self._cache = {}  # Simple cache\n",
    "        self.valid_keys = {\"enterprise_key_123\"}\n",
    "        self.metrics = {\n",
    "            'total_requests': 0,\n",
    "            'cache_hits': 0,\n",
    "            'auth_failures': 0,\n",
    "            'total_cost_saved': 0.0\n",
    "        }\n",
    "    \n",
    "    def complete(self, prompt: str, api_key: str = None, **kwargs) -> str:\n",
    "        self.metrics['total_requests'] += 1\n",
    "        \n",
    "        # 1. Authentication\n",
    "        if api_key not in self.valid_keys:\n",
    "            self.metrics['auth_failures'] += 1\n",
    "            raise Exception(\"üîí Access denied\")\n",
    "        \n",
    "        # 2. Caching check\n",
    "        cache_key = hashlib.md5(prompt.encode()).hexdigest()\n",
    "        if cache_key in self._cache:\n",
    "            self.metrics['cache_hits'] += 1\n",
    "            self.metrics['total_cost_saved'] += 0.001  # Estimated savings\n",
    "            return f\"‚ö° [CACHED] {self._cache[cache_key]}\"\n",
    "        \n",
    "        # 3. Real service call\n",
    "        result = self._real_service.complete(prompt, **kwargs)\n",
    "        self._cache[cache_key] = result\n",
    "        \n",
    "        return f\"üöÄ [LIVE] {result}\"\n",
    "    \n",
    "    def get_dashboard(self):\n",
    "        \"\"\"Enterprise monitoring dashboard\"\"\"\n",
    "        hit_rate = (self.metrics['cache_hits'] / max(1, self.metrics['total_requests'])) * 100\n",
    "        \n",
    "        dashboard = f\"\"\"\n",
    "üè¢ ENTERPRISE LLM PROXY DASHBOARD\n",
    "=====================================\n",
    "üìä Total Requests: {self.metrics['total_requests']}\n",
    "‚ö° Cache Hit Rate: {hit_rate:.1f}%\n",
    "üîí Auth Failures: {self.metrics['auth_failures']}\n",
    "üí∞ Cost Saved: ${self.metrics['total_cost_saved']:.3f}\n",
    "üéØ Status: {'‚úÖ HEALTHY' if hit_rate > 50 else '‚ö†Ô∏è LOW CACHE EFFICIENCY'}\n",
    "        \"\"\"\n",
    "        return dashboard\n",
    "\n",
    "print(\"‚úÖ Enterprise proxy ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Demo: Enterprise Dashboard & ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enterprise proxy\n",
    "real_service = OpenAIService()\n",
    "enterprise_proxy = EnterpriseLLMProxy(real_service)\n",
    "\n",
    "print(\"üè¢ Enterprise LLM Proxy Demo\\n\")\n",
    "\n",
    "# Simulate enterprise usage\n",
    "enterprise_requests = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain cloud computing\", \n",
    "    \"What is machine learning?\",  # Repeat for cache\n",
    "    \"Best database practices\",\n",
    "    \"Explain cloud computing\",     # Repeat for cache\n",
    "    \"What is machine learning?\",  # Repeat for cache\n",
    "]\n",
    "\n",
    "# Process requests\n",
    "for i, prompt in enumerate(enterprise_requests, 1):\n",
    "    try:\n",
    "        response = enterprise_proxy.complete(prompt, api_key=\"enterprise_key_123\")\n",
    "        print(f\"{i}. {response[:70]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"{i}. ‚ùå {e}\")\n",
    "\n",
    "# Show enterprise dashboard\n",
    "print(enterprise_proxy.get_dashboard())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways & Business Impact\n",
    "\n",
    "### üéØ **Proxy Pattern Benefits in LLM Systems**\n",
    "\n",
    "| Feature | Business Impact | Implementation |\n",
    "|---------|----------------|----------------|\n",
    "| **Smart Caching** | 60-80% cost reduction | Cache frequently requested prompts |\n",
    "| **Authentication** | Security & access control | API key validation |\n",
    "| **Rate Limiting** | Prevent abuse & cost overruns | Request throttling |\n",
    "| **Load Balancing** | Optimize cost & performance | Multi-provider routing |\n",
    "| **Monitoring** | Operational visibility | Real-time metrics |\n",
    "\n",
    "### üí° **Real-World Applications**\n",
    "- **Enterprise AI Gateways**: Single control point for all LLM access\n",
    "- **Cost Optimization**: Intelligent caching and provider selection  \n",
    "- **Security**: Content filtering and audit logging\n",
    "- **Compliance**: Access controls and usage tracking\n",
    "\n",
    "### üöÄ **Next Steps**\n",
    "1. Implement circuit breakers for resilience\n",
    "2. Add semantic caching for better hit rates\n",
    "3. Integrate with monitoring systems\n",
    "4. Scale to production with async/await patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}