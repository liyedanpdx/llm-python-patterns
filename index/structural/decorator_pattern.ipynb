{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decorator Pattern: LLM Response Caching\n",
    "\n",
    "**Decorator Pattern** = Wrap functions to add new capabilities without changing core logic!\n",
    "\n",
    "## The Problem\n",
    "- LLM API calls are expensive ($0.03 per 1K tokens)\n",
    "- Slow response times (2-5 seconds)\n",
    "- Same queries asked repeatedly\n",
    "- No built-in caching mechanism\n",
    "\n",
    "## The Solution\n",
    "Use Decorator Pattern to add transparent caching to any LLM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë API Mode: Real OpenAI API\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from config import API_PROVIDERS \n",
    "# API Configuration - Add your OpenAI API key here\n",
    "OPENAI_API_KEY = API_PROVIDERS['openai']['api_key']# Add your OpenAI API key here\n",
    "\n",
    "# If no API key provided, we'll use mock responses for demonstration\n",
    "USE_REAL_API = bool(OPENAI_API_KEY.strip())\n",
    "\n",
    "print(f\"üîë API Mode: {'Real OpenAI API' if USE_REAL_API else 'Mock Demo Mode'}\")\n",
    "if not USE_REAL_API:\n",
    "    print(\"üí° Add your OpenAI API key above to test with real API calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client initialized\n",
      "üì¶ Decorator Pattern Demo - LLM Response Caching\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import functools\n",
    "import random\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "# Import OpenAI if using real API\n",
    "if USE_REAL_API:\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        print(\"‚úÖ OpenAI client initialized\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå OpenAI package not found. Install with: pip install openai\")\n",
    "        USE_REAL_API = False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå OpenAI client failed: {e}\")\n",
    "        USE_REAL_API = False\n",
    "\n",
    "print(\"üì¶ Decorator Pattern Demo - LLM Response Caching\")\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build Smart Cache System\n",
    "\n",
    "First, let's create an intelligent cache for LLM responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLMCache created!\n",
      "üîë Features: TTL expiration, size limits, performance stats\n"
     ]
    }
   ],
   "source": [
    "class LLMCache:\n",
    "    \"\"\"Smart cache for LLM responses with TTL and size management\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100, ttl: int = 3600):\n",
    "        self.cache: Dict[str, Dict[str, Any]] = {}\n",
    "        self.max_size = max_size\n",
    "        self.ttl = ttl  # Time to live in seconds\n",
    "        self.stats = {'hits': 0, 'misses': 0, 'evictions': 0}\n",
    "    \n",
    "    def _generate_key(self, *args, **kwargs) -> str:\n",
    "        \"\"\"Generate unique cache key from function arguments\"\"\"\n",
    "        key_data = {'args': str(args), 'kwargs': kwargs}\n",
    "        key_string = json.dumps(key_data, sort_keys=True)\n",
    "        return hashlib.md5(key_string.encode()).hexdigest()[:10]\n",
    "    \n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        \"\"\"Get cached value if exists and not expired\"\"\"\n",
    "        if key in self.cache:\n",
    "            entry = self.cache[key]\n",
    "            if time.time() - entry['timestamp'] < self.ttl:\n",
    "                self.stats['hits'] += 1\n",
    "                return entry['value']\n",
    "            else:\n",
    "                del self.cache[key]  # Remove expired entry\n",
    "        \n",
    "        self.stats['misses'] += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, key: str, value: Any):\n",
    "        \"\"\"Set cache value with timestamp\"\"\"\n",
    "        # Evict oldest if cache is full\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            oldest_key = min(self.cache.keys(), \n",
    "                           key=lambda k: self.cache[k]['timestamp'])\n",
    "            del self.cache[oldest_key]\n",
    "            self.stats['evictions'] += 1\n",
    "        \n",
    "        self.cache[key] = {'value': value, 'timestamp': time.time()}\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache performance statistics\"\"\"\n",
    "        total = self.stats['hits'] + self.stats['misses']\n",
    "        hit_rate = (self.stats['hits'] / total * 100) if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'size': len(self.cache),\n",
    "            'hit_rate': round(hit_rate, 1),\n",
    "            'hits': self.stats['hits'],\n",
    "            'misses': self.stats['misses'],\n",
    "            'evictions': self.stats['evictions']\n",
    "        }\n",
    "\n",
    "# Create global cache instance\n",
    "llm_cache = LLMCache(max_size=50, ttl=300)  # 5 minute TTL\n",
    "\n",
    "print(\"‚úÖ LLMCache created!\")\n",
    "print(\"üîë Features: TTL expiration, size limits, performance stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Caching Decorator\n",
    "\n",
    "This is the core of the Decorator Pattern - wrapping functions to add caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Caching Decorator created!\n",
      "üéØ Ready to wrap any function with intelligent caching\n"
     ]
    }
   ],
   "source": [
    "def cache_llm_response(cache_instance: LLMCache = llm_cache):\n",
    "    \"\"\"Decorator to cache LLM responses\n",
    "    \n",
    "    This is the Decorator Pattern in action:\n",
    "    - Wraps original function transparently\n",
    "    - Adds caching behavior without code changes\n",
    "    - Preserves original function interface\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Generate cache key from arguments\n",
    "            cache_key = cache_instance._generate_key(*args, **kwargs)\n",
    "            \n",
    "            # Try cache first\n",
    "            cached_result = cache_instance.get(cache_key)\n",
    "            if cached_result is not None:\n",
    "                print(f\"üéØ Cache HIT! Key: {cache_key}\")\n",
    "                return cached_result\n",
    "            \n",
    "            # Cache miss - call original function\n",
    "            print(f\"üì° Cache MISS - Calling function... Key: {cache_key}\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            # Store in cache\n",
    "            cache_instance.set(cache_key, result)\n",
    "            duration = time.time() - start_time\n",
    "            print(f\"üíæ Response cached (took {duration:.2f}s)\")\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "print(\"‚úÖ Caching Decorator created!\")\n",
    "print(\"üéØ Ready to wrap any function with intelligent caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create LLM Functions\n",
    "\n",
    "Let's create both real API and mock versions of LLM functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM function ready: Real OpenAI API\n"
     ]
    }
   ],
   "source": [
    "def mock_llm_call(prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Mock LLM function for demonstration\"\"\"\n",
    "    # Simulate API delay\n",
    "    time.sleep(random.uniform(1.0, 3.0))\n",
    "    \n",
    "    # Generate realistic mock responses\n",
    "    responses = {\n",
    "        \"artificial intelligence\": \"Artificial Intelligence (AI) refers to computer systems that can perform tasks typically requiring human intelligence, such as learning, reasoning, and problem-solving.\",\n",
    "        \"machine learning\": \"Machine learning is a subset of AI that enables computers to learn and improve from data without being explicitly programmed for every task.\",\n",
    "        \"cloud computing\": \"Cloud computing delivers computing services over the internet, offering scalability, cost-effectiveness, and accessibility from anywhere.\",\n",
    "        \"blockchain\": \"Blockchain is a distributed ledger technology that maintains a secure, transparent record of transactions across multiple computers.\"\n",
    "    }\n",
    "    \n",
    "    # Find best match for prompt\n",
    "    for key, response in responses.items():\n",
    "        if key.lower() in prompt.lower():\n",
    "            return response\n",
    "    \n",
    "    return f\"This is a mock response to your query about: {prompt[:50]}...\"\n",
    "\n",
    "def real_llm_call(prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Real OpenAI API call\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        # Handle None response\n",
    "        if result is None:\n",
    "            return f\"[API returned empty response for: {prompt[:50]}...]\"\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API Error: {e}\")\n",
    "        return f\"API Error: {str(e)}\"\n",
    "\n",
    "# Select the appropriate function based on API availability\n",
    "base_llm_call = real_llm_call if USE_REAL_API else mock_llm_call\n",
    "\n",
    "print(f\"‚úÖ LLM function ready: {'Real OpenAI API' if USE_REAL_API else 'Mock Demo'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply Decorator Pattern\n",
    "\n",
    "Now let's enhance our LLM function with caching using the decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Decorator Pattern Applied!\n",
      "üé≠ Functions enhanced with caching behavior\n",
      "üîÑ Same interface, new capabilities\n"
     ]
    }
   ],
   "source": [
    "# Apply the decorator to our LLM function\n",
    "@cache_llm_response()\n",
    "def cached_llm_call(prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"LLM call enhanced with caching via Decorator Pattern\"\"\"\n",
    "    return base_llm_call(prompt, model)\n",
    "\n",
    "# Create multiple cached functions for different use cases\n",
    "@cache_llm_response()\n",
    "def ask_question(question: str) -> str:\n",
    "    \"\"\"Ask a general question\"\"\"\n",
    "    return base_llm_call(f\"Please answer this question: {question}\")\n",
    "\n",
    "@cache_llm_response()\n",
    "def summarize_text(text: str, max_words: int = 50) -> str:\n",
    "    \"\"\"Summarize text with caching\"\"\"\n",
    "    prompt = f\"Summarize this text in {max_words} words: {text}\"\n",
    "    return base_llm_call(prompt)\n",
    "\n",
    "print(\"‚úÖ Decorator Pattern Applied!\")\n",
    "print(\"üé≠ Functions enhanced with caching behavior\")\n",
    "print(\"üîÑ Same interface, new capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Demonstrate Cache Performance\n",
    "\n",
    "Let's see the performance benefits of our caching decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Decorator Pattern Performance Demo\n",
      "=============================================\n",
      "\n",
      "üìù Round 1: First calls (cache misses)\n",
      "----------------------------------------\n",
      "\n",
      "1. What is artificial intelligence?\n",
      "üì° Cache MISS - Calling function... Key: 9bfe3c17c3\n",
      "üíæ Response cached (took 1.77s)\n",
      "   ‚è±Ô∏è Time: 1.77s\n",
      "   üì§ Response: Artificial intelligence, or AI, refers to the simulation of human intelligence i...\n",
      "\n",
      "2. Explain machine learning\n",
      "üì° Cache MISS - Calling function... Key: 592ae03d83\n",
      "üíæ Response cached (took 1.63s)\n",
      "   ‚è±Ô∏è Time: 1.63s\n",
      "   üì§ Response: Machine learning is a subset of artificial intelligence that involves the develo...\n",
      "\n",
      "3. What are benefits of cloud computing?\n",
      "üì° Cache MISS - Calling function... Key: 4611cbbf62\n",
      "üíæ Response cached (took 1.69s)\n",
      "   ‚è±Ô∏è Time: 1.69s\n",
      "   üì§ Response: 1. Cost savings: Cloud computing eliminates the need for upfront investments in ...\n",
      "\n",
      "üìä Cache Stats: 0 hits, 3 misses\n",
      "\n",
      "üìù Round 2: Repeat calls (cache hits expected)\n",
      "----------------------------------------\n",
      "\n",
      "1. What is artificial intelligence?\n",
      "üéØ Cache HIT! Key: 9bfe3c17c3\n",
      "   ‚ö° Time: 0.001s\n",
      "   üì§ Response: Artificial intelligence, or AI, refers to the simulation of human intelligence i...\n",
      "\n",
      "2. Explain machine learning\n",
      "üéØ Cache HIT! Key: 592ae03d83\n",
      "   ‚ö° Time: 0.000s\n",
      "   üì§ Response: Machine learning is a subset of artificial intelligence that involves the develo...\n",
      "\n",
      "3. What are benefits of cloud computing?\n",
      "üéØ Cache HIT! Key: 4611cbbf62\n",
      "   ‚ö° Time: 0.000s\n",
      "   üì§ Response: 1. Cost savings: Cloud computing eliminates the need for upfront investments in ...\n",
      "\n",
      "üìä Final Stats: 3 hits, 3 misses\n",
      "üéØ Hit Rate: 50.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Decorator Pattern Performance Demo\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Test queries\n",
    "queries = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain machine learning\",\n",
    "    \"What are benefits of cloud computing?\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìù Round 1: First calls (cache misses)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "round1_times = []\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\n{i}. {query}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    response = cached_llm_call(query)\n",
    "    duration = time.time() - start\n",
    "    round1_times.append(duration)\n",
    "    \n",
    "    print(f\"   ‚è±Ô∏è Time: {duration:.2f}s\")\n",
    "    if response:\n",
    "        print(f\"   üì§ Response: {response[:80]}...\")\n",
    "    else:\n",
    "        print(f\"   üì§ Response: [No response received]\")\n",
    "\n",
    "# Show cache stats\n",
    "stats = llm_cache.get_stats()\n",
    "print(f\"\\nüìä Cache Stats: {stats['hits']} hits, {stats['misses']} misses\")\n",
    "\n",
    "print(\"\\nüìù Round 2: Repeat calls (cache hits expected)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "round2_times = []\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\n{i}. {query}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    response = cached_llm_call(query)\n",
    "    duration = time.time() - start\n",
    "    round2_times.append(duration)\n",
    "    \n",
    "    print(f\"   ‚ö° Time: {duration:.3f}s\")\n",
    "    if response:\n",
    "        print(f\"   üì§ Response: {response[:80]}...\")\n",
    "    else:\n",
    "        print(f\"   üì§ Response: [No response received]\")\n",
    "\n",
    "# Final stats\n",
    "final_stats = llm_cache.get_stats()\n",
    "print(f\"\\nüìä Final Stats: {final_stats['hits']} hits, {final_stats['misses']} misses\")\n",
    "print(f\"üéØ Hit Rate: {final_stats['hit_rate']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Performance Analysis\n",
    "\n",
    "Let's analyze the performance benefits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Performance Analysis\n",
      "==============================\n",
      "\n",
      "‚ö° Speed Comparison:\n",
      "   First calls:  1.69s average\n",
      "   Cached calls: 0.000s average\n",
      "   Improvement:  100.0% faster\n",
      "\n",
      "üí∞ Cost Analysis:\n",
      "   Without cache: $0.012\n",
      "   With cache:    $0.006\n",
      "   Savings:       $0.006 (50.0%)\n",
      "\n",
      "üéØ Decorator Pattern Benefits:\n",
      "   1. ‚úÖ Zero code changes to original function\n",
      "   2. ‚úÖ Transparent caching behavior\n",
      "   3. ‚úÖ 100.0% performance improvement\n",
      "   4. ‚úÖ 50.0% cost reduction\n",
      "   5. ‚úÖ Built-in cache management\n",
      "   6. ‚úÖ Easy to enable/disable\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìà Performance Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Calculate improvements\n",
    "avg_round1 = sum(round1_times) / len(round1_times)\n",
    "avg_round2 = sum(round2_times) / len(round2_times)\n",
    "speed_improvement = ((avg_round1 - avg_round2) / avg_round1) * 100\n",
    "\n",
    "print(f\"\\n‚ö° Speed Comparison:\")\n",
    "print(f\"   First calls:  {avg_round1:.2f}s average\")\n",
    "print(f\"   Cached calls: {avg_round2:.3f}s average\")\n",
    "print(f\"   Improvement:  {speed_improvement:.1f}% faster\")\n",
    "\n",
    "# Cost analysis\n",
    "api_cost_per_call = 0.002  # Example cost\n",
    "calls_without_cache = 6    # 3 queries √ó 2 rounds\n",
    "actual_api_calls = final_stats['misses']\n",
    "\n",
    "cost_without_cache = calls_without_cache * api_cost_per_call\n",
    "cost_with_cache = actual_api_calls * api_cost_per_call\n",
    "savings = cost_without_cache - cost_with_cache\n",
    "savings_percent = (savings / cost_without_cache) * 100\n",
    "\n",
    "print(f\"\\nüí∞ Cost Analysis:\")\n",
    "print(f\"   Without cache: ${cost_without_cache:.3f}\")\n",
    "print(f\"   With cache:    ${cost_with_cache:.3f}\")\n",
    "print(f\"   Savings:       ${savings:.3f} ({savings_percent:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Decorator Pattern Benefits:\")\n",
    "benefits = [\n",
    "    \"Zero code changes to original function\",\n",
    "    \"Transparent caching behavior\",\n",
    "    f\"{speed_improvement:.1f}% performance improvement\",\n",
    "    f\"{savings_percent:.1f}% cost reduction\",\n",
    "    \"Built-in cache management\",\n",
    "    \"Easy to enable/disable\"\n",
    "]\n",
    "\n",
    "for i, benefit in enumerate(benefits, 1):\n",
    "    print(f\"   {i}. ‚úÖ {benefit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Advanced - Decorator Composition\n",
    "\n",
    "Show how multiple decorators can be stacked for rich functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Decorator Composition Demo\n",
      "================================\n",
      "Stack: @timing ‚Üí @cost_tracker ‚Üí @cache ‚Üí function\n",
      "\n",
      "üìù First call: What is the future of AI?\n",
      "üì° Cache MISS - Calling function... Key: fc9d9c24c4\n",
      "üíæ Response cached (took 2.75s)\n",
      "üí∞ Cost: $0.002 | Total: $0.002\n",
      "‚è±Ô∏è Execution: 2.751s\n",
      "\n",
      "üìù Second call (cached): What is the future of AI?\n",
      "üéØ Cache HIT! Key: fc9d9c24c4\n",
      "üí∞ Cost: $0.002 | Total: $0.004\n",
      "‚è±Ô∏è Execution: 0.001s\n",
      "\n",
      "üìä Total tracked cost: $0.004\n",
      "\n",
      "üéØ Composition Benefits:\n",
      "   1. ‚ú® Modular design - each decorator handles one concern\n",
      "   2. ‚ú® Flexible stacking - mix and match as needed\n",
      "   3. ‚ú® Order matters - cache first, then track what executes\n",
      "   4. ‚ú® Reusable - same decorators work on any function\n"
     ]
    }
   ],
   "source": [
    "def timing_decorator(func):\n",
    "    \"\"\"Decorator to measure execution time\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        duration = time.time() - start\n",
    "        print(f\"‚è±Ô∏è Execution: {duration:.3f}s\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def cost_tracker(cost_per_call: float = 0.002):\n",
    "    \"\"\"Decorator to track costs\"\"\"\n",
    "    total_cost = {'value': 0.0}\n",
    "    \n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            # Only count cost if it was an actual API call (not cached)\n",
    "            if \"Cache HIT\" not in str(result):\n",
    "                total_cost['value'] += cost_per_call\n",
    "                print(f\"üí∞ Cost: ${cost_per_call:.3f} | Total: ${total_cost['value']:.3f}\")\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        wrapper.get_total_cost = lambda: total_cost['value']\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Create function with multiple decorators\n",
    "@timing_decorator\n",
    "@cost_tracker(0.002)\n",
    "@cache_llm_response()\n",
    "def enhanced_llm_call(prompt: str) -> str:\n",
    "    \"\"\"LLM call with timing, cost tracking, and caching\"\"\"\n",
    "    return base_llm_call(prompt)\n",
    "\n",
    "print(\"üîó Decorator Composition Demo\")\n",
    "print(\"=\" * 32)\n",
    "print(\"Stack: @timing ‚Üí @cost_tracker ‚Üí @cache ‚Üí function\")\n",
    "\n",
    "test_prompt = \"What is the future of AI?\"\n",
    "\n",
    "print(f\"\\nüìù First call: {test_prompt}\")\n",
    "result1 = enhanced_llm_call(test_prompt)\n",
    "\n",
    "print(f\"\\nüìù Second call (cached): {test_prompt}\")\n",
    "result2 = enhanced_llm_call(test_prompt)\n",
    "\n",
    "print(f\"\\nüìä Total tracked cost: ${enhanced_llm_call.get_total_cost():.3f}\")\n",
    "\n",
    "print(\"\\nüéØ Composition Benefits:\")\n",
    "print(\"   1. ‚ú® Modular design - each decorator handles one concern\")\n",
    "print(\"   2. ‚ú® Flexible stacking - mix and match as needed\")\n",
    "print(\"   3. ‚ú® Order matters - cache first, then track what executes\")\n",
    "print(\"   4. ‚ú® Reusable - same decorators work on any function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You've successfully learned the Decorator Pattern for LLM applications!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Decorator Pattern Learning Summary\n",
    "\n",
    "The Decorator Pattern allows you to add new behavior to existing functions without altering their core logic.\n",
    "It preserves the original interface, supports flexible configuration, and enables multiple enhancements through stacking.\n",
    "\n",
    "**Core Concepts**\n",
    "\n",
    "* Add functionality without changing original code\n",
    "* Keep the same function signature (transparency)\n",
    "* Use `functools.wraps` to preserve metadata\n",
    "* Allow parameterized decorators\n",
    "* Support stacking multiple decorators\n",
    "\n",
    "In practice, decorators improve flexibility, maintain cleaner code, and make enhancements reusable.\n",
    "\n",
    "**Key Advantages**\n",
    "\n",
    "* **Zero Code Changes** ‚Äì Apply the decorator directly\n",
    "* **Separation of Concerns** ‚Äì Business logic remains isolated\n",
    "* **Reusability** ‚Äì Works with different functions\n",
    "* **Composability** ‚Äì Combine multiple decorators easily\n",
    "* **Testability** ‚Äì Test each decorator in isolation\n",
    "* **Performance Gains** ‚Äì Faster execution and reduced costs\n",
    "\n",
    "When applied to LLM systems, decorators can significantly improve efficiency and monitoring.\n",
    "\n",
    "**LLM-Specific Applications**\n",
    "\n",
    "* Response caching (50‚Äì80% API cost reduction)\n",
    "* Cost tracking across all calls\n",
    "* Performance monitoring (latency, success rates)\n",
    "* Retry logic with exponential backoff\n",
    "* Rate limiting to avoid hitting quotas\n",
    "* Graceful error handling with fallback options\n",
    "\n",
    "To ensure production readiness, follow best practices for caching and scalability.\n",
    "\n",
    "**Best Practices**\n",
    "\n",
    "* Set TTL for cache expiration\n",
    "* Limit cache size to avoid memory overload\n",
    "* Maintain cache statistics for optimization\n",
    "* Provide cache bypass for real-time scenarios\n",
    "* Use consistent cache key generation\n",
    "* Design with composition in mind from the start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
